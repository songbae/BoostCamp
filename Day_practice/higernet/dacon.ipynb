{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "import cv2\r\n",
    "import random\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "import albumentations as A\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\r\n",
    "from typing import Tuple, List, Sequence, Callable\r\n",
    "\r\n",
    "import timm\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch_optimizer as optim\r\n",
    "import torchvision.transforms as tfms\r\n",
    "from torchinfo import summary\r\n",
    "from torch.utils.data import DataLoader, Dataset\r\n",
    "import gc\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "gc.collect()\r\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir='./data/'\n",
    "train_img_path = os.path.join(main_dir, 'train_imgs')\n",
    "test_img_path = os.path.join(main_dir, 'test_imgs')\n",
    "meta_info_dir = os.path.join(main_dir, 'train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(4195, 49)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(meta_info_dir)\r\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sector</th>\n      <th>image</th>\n      <th>nose_x</th>\n      <th>nose_y</th>\n      <th>left_eye_x</th>\n      <th>left_eye_y</th>\n      <th>right_eye_x</th>\n      <th>right_eye_y</th>\n      <th>left_ear_x</th>\n      <th>left_ear_y</th>\n      <th>...</th>\n      <th>right_palm_x</th>\n      <th>right_palm_y</th>\n      <th>spine2(back)_x</th>\n      <th>spine2(back)_y</th>\n      <th>spine1(waist)_x</th>\n      <th>spine1(waist)_y</th>\n      <th>left_instep_x</th>\n      <th>left_instep_y</th>\n      <th>right_instep_x</th>\n      <th>right_instep_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001A</td>\n      <td>001-1-1-01-Z17_A-0000001.jpg</td>\n      <td>1046.389631</td>\n      <td>344.757881</td>\n      <td>1041.655294</td>\n      <td>329.820225</td>\n      <td>1059.429507</td>\n      <td>334.484230</td>\n      <td>1020.117796</td>\n      <td>338.890539</td>\n      <td>...</td>\n      <td>1067.000000</td>\n      <td>335.000000</td>\n      <td>1019.484230</td>\n      <td>455.000000</td>\n      <td>1026.515770</td>\n      <td>514.054730</td>\n      <td>998.578836</td>\n      <td>826.718013</td>\n      <td>1063.204067</td>\n      <td>838.827465</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001A</td>\n      <td>001-1-1-01-Z17_A-0000003.jpg</td>\n      <td>1069.850679</td>\n      <td>340.711494</td>\n      <td>1058.608552</td>\n      <td>324.593690</td>\n      <td>1075.242111</td>\n      <td>325.593690</td>\n      <td>1041.422997</td>\n      <td>331.694815</td>\n      <td>...</td>\n      <td>1081.187380</td>\n      <td>323.000000</td>\n      <td>1046.953248</td>\n      <td>454.062706</td>\n      <td>1058.766231</td>\n      <td>508.797029</td>\n      <td>1002.265676</td>\n      <td>699.062706</td>\n      <td>1066.376234</td>\n      <td>841.499445</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001A</td>\n      <td>001-1-1-01-Z17_A-0000005.jpg</td>\n      <td>1084.475902</td>\n      <td>337.000008</td>\n      <td>1078.717997</td>\n      <td>323.757889</td>\n      <td>1095.648412</td>\n      <td>325.242119</td>\n      <td>1061.039884</td>\n      <td>329.351571</td>\n      <td>...</td>\n      <td>1101.000000</td>\n      <td>334.000000</td>\n      <td>1044.538960</td>\n      <td>442.054730</td>\n      <td>1052.844144</td>\n      <td>495.890539</td>\n      <td>989.437847</td>\n      <td>808.757889</td>\n      <td>1066.071417</td>\n      <td>841.749554</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001A</td>\n      <td>001-1-1-01-Z17_A-0000007.jpg</td>\n      <td>1042.320047</td>\n      <td>361.452689</td>\n      <td>1037.907194</td>\n      <td>344.117804</td>\n      <td>1050.328382</td>\n      <td>353.913729</td>\n      <td>1016.844144</td>\n      <td>340.913737</td>\n      <td>...</td>\n      <td>1057.406318</td>\n      <td>372.461040</td>\n      <td>982.937294</td>\n      <td>458.109462</td>\n      <td>990.375124</td>\n      <td>507.624866</td>\n      <td>1001.305177</td>\n      <td>829.233767</td>\n      <td>1159.516499</td>\n      <td>599.389997</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>001A</td>\n      <td>001-1-1-01-Z17_A-0000009.jpg</td>\n      <td>1058.046395</td>\n      <td>343.164191</td>\n      <td>1046.717997</td>\n      <td>331.703163</td>\n      <td>1058.132650</td>\n      <td>331.781079</td>\n      <td>1031.258806</td>\n      <td>338.593690</td>\n      <td>...</td>\n      <td>1069.648429</td>\n      <td>334.109461</td>\n      <td>1024.843791</td>\n      <td>453.687572</td>\n      <td>1034.391088</td>\n      <td>510.843791</td>\n      <td>998.625231</td>\n      <td>805.218921</td>\n      <td>1059.625956</td>\n      <td>839.765102</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4190</th>\n      <td>642E</td>\n      <td>642-2-4-31-Z148_E-0000023.jpg</td>\n      <td>637.251052</td>\n      <td>781.342260</td>\n      <td>1117.135818</td>\n      <td>370.667301</td>\n      <td>1097.123228</td>\n      <td>330.642200</td>\n      <td>1095.455539</td>\n      <td>355.657874</td>\n      <td>...</td>\n      <td>762.746552</td>\n      <td>862.643292</td>\n      <td>740.504148</td>\n      <td>890.141117</td>\n      <td>805.273267</td>\n      <td>541.608226</td>\n      <td>630.997090</td>\n      <td>612.486105</td>\n      <td>616.404617</td>\n      <td>803.439462</td>\n    </tr>\n    <tr>\n      <th>4191</th>\n      <td>642E</td>\n      <td>642-2-4-31-Z148_E-0000025.jpg</td>\n      <td>637.251052</td>\n      <td>781.342260</td>\n      <td>1117.135818</td>\n      <td>370.667301</td>\n      <td>1097.123228</td>\n      <td>330.642200</td>\n      <td>1095.455539</td>\n      <td>355.657874</td>\n      <td>...</td>\n      <td>762.746552</td>\n      <td>862.643292</td>\n      <td>740.504148</td>\n      <td>890.141117</td>\n      <td>805.273267</td>\n      <td>541.608226</td>\n      <td>630.997090</td>\n      <td>612.486105</td>\n      <td>616.404617</td>\n      <td>803.439462</td>\n    </tr>\n    <tr>\n      <th>4192</th>\n      <td>642E</td>\n      <td>642-2-4-31-Z148_E-0000027.jpg</td>\n      <td>637.251052</td>\n      <td>781.342260</td>\n      <td>1117.135818</td>\n      <td>370.667301</td>\n      <td>1097.123228</td>\n      <td>330.642200</td>\n      <td>1095.455539</td>\n      <td>355.657874</td>\n      <td>...</td>\n      <td>762.746552</td>\n      <td>862.643292</td>\n      <td>740.504148</td>\n      <td>890.141117</td>\n      <td>805.273267</td>\n      <td>541.608226</td>\n      <td>630.997090</td>\n      <td>612.486105</td>\n      <td>616.404617</td>\n      <td>803.439462</td>\n    </tr>\n    <tr>\n      <th>4193</th>\n      <td>642E</td>\n      <td>642-2-4-31-Z148_E-0000029.jpg</td>\n      <td>637.251052</td>\n      <td>781.342260</td>\n      <td>1117.135818</td>\n      <td>370.667301</td>\n      <td>1097.123228</td>\n      <td>330.642200</td>\n      <td>1095.455539</td>\n      <td>355.657874</td>\n      <td>...</td>\n      <td>762.746552</td>\n      <td>862.643292</td>\n      <td>740.504148</td>\n      <td>890.141117</td>\n      <td>805.273267</td>\n      <td>541.608226</td>\n      <td>630.997090</td>\n      <td>612.486105</td>\n      <td>616.404617</td>\n      <td>803.439462</td>\n    </tr>\n    <tr>\n      <th>4194</th>\n      <td>642E</td>\n      <td>642-2-4-31-Z148_E-0000031.jpg</td>\n      <td>637.251052</td>\n      <td>781.342260</td>\n      <td>1117.135818</td>\n      <td>370.667301</td>\n      <td>1097.123228</td>\n      <td>330.642200</td>\n      <td>1095.455539</td>\n      <td>355.657874</td>\n      <td>...</td>\n      <td>762.746552</td>\n      <td>862.643292</td>\n      <td>740.504148</td>\n      <td>890.141117</td>\n      <td>805.273267</td>\n      <td>541.608226</td>\n      <td>630.997090</td>\n      <td>612.486105</td>\n      <td>616.404617</td>\n      <td>803.439462</td>\n    </tr>\n  </tbody>\n</table>\n<p>4195 rows × 50 columns</p>\n</div>",
      "text/plain": "     sector                          image       nose_x      nose_y  \\\n0      001A   001-1-1-01-Z17_A-0000001.jpg  1046.389631  344.757881   \n1      001A   001-1-1-01-Z17_A-0000003.jpg  1069.850679  340.711494   \n2      001A   001-1-1-01-Z17_A-0000005.jpg  1084.475902  337.000008   \n3      001A   001-1-1-01-Z17_A-0000007.jpg  1042.320047  361.452689   \n4      001A   001-1-1-01-Z17_A-0000009.jpg  1058.046395  343.164191   \n...     ...                            ...          ...         ...   \n4190   642E  642-2-4-31-Z148_E-0000023.jpg   637.251052  781.342260   \n4191   642E  642-2-4-31-Z148_E-0000025.jpg   637.251052  781.342260   \n4192   642E  642-2-4-31-Z148_E-0000027.jpg   637.251052  781.342260   \n4193   642E  642-2-4-31-Z148_E-0000029.jpg   637.251052  781.342260   \n4194   642E  642-2-4-31-Z148_E-0000031.jpg   637.251052  781.342260   \n\n       left_eye_x  left_eye_y  right_eye_x  right_eye_y   left_ear_x  \\\n0     1041.655294  329.820225  1059.429507   334.484230  1020.117796   \n1     1058.608552  324.593690  1075.242111   325.593690  1041.422997   \n2     1078.717997  323.757889  1095.648412   325.242119  1061.039884   \n3     1037.907194  344.117804  1050.328382   353.913729  1016.844144   \n4     1046.717997  331.703163  1058.132650   331.781079  1031.258806   \n...           ...         ...          ...          ...          ...   \n4190  1117.135818  370.667301  1097.123228   330.642200  1095.455539   \n4191  1117.135818  370.667301  1097.123228   330.642200  1095.455539   \n4192  1117.135818  370.667301  1097.123228   330.642200  1095.455539   \n4193  1117.135818  370.667301  1097.123228   330.642200  1095.455539   \n4194  1117.135818  370.667301  1097.123228   330.642200  1095.455539   \n\n      left_ear_y  ...  right_palm_x  right_palm_y  spine2(back)_x  \\\n0     338.890539  ...   1067.000000    335.000000     1019.484230   \n1     331.694815  ...   1081.187380    323.000000     1046.953248   \n2     329.351571  ...   1101.000000    334.000000     1044.538960   \n3     340.913737  ...   1057.406318    372.461040      982.937294   \n4     338.593690  ...   1069.648429    334.109461     1024.843791   \n...          ...  ...           ...           ...             ...   \n4190  355.657874  ...    762.746552    862.643292      740.504148   \n4191  355.657874  ...    762.746552    862.643292      740.504148   \n4192  355.657874  ...    762.746552    862.643292      740.504148   \n4193  355.657874  ...    762.746552    862.643292      740.504148   \n4194  355.657874  ...    762.746552    862.643292      740.504148   \n\n      spine2(back)_y  spine1(waist)_x  spine1(waist)_y  left_instep_x  \\\n0         455.000000      1026.515770       514.054730     998.578836   \n1         454.062706      1058.766231       508.797029    1002.265676   \n2         442.054730      1052.844144       495.890539     989.437847   \n3         458.109462       990.375124       507.624866    1001.305177   \n4         453.687572      1034.391088       510.843791     998.625231   \n...              ...              ...              ...            ...   \n4190      890.141117       805.273267       541.608226     630.997090   \n4191      890.141117       805.273267       541.608226     630.997090   \n4192      890.141117       805.273267       541.608226     630.997090   \n4193      890.141117       805.273267       541.608226     630.997090   \n4194      890.141117       805.273267       541.608226     630.997090   \n\n      left_instep_y  right_instep_x  right_instep_y  \n0        826.718013     1063.204067      838.827465  \n1        699.062706     1066.376234      841.499445  \n2        808.757889     1066.071417      841.749554  \n3        829.233767     1159.516499      599.389997  \n4        805.218921     1059.625956      839.765102  \n...             ...             ...             ...  \n4190     612.486105      616.404617      803.439462  \n4191     612.486105      616.404617      803.439462  \n4192     612.486105      616.404617      803.439462  \n4193     612.486105      616.404617      803.439462  \n4194     612.486105      616.404617      803.439462  \n\n[4195 rows x 50 columns]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def making_sector_label(image_name):\r\n",
    "  pose = image_name.split('-')\r\n",
    "  cam_dir = pose[4].split('_')[1]\r\n",
    "  sector_name = pose[0] + cam_dir\r\n",
    "  return sector_name\r\n",
    "\r\n",
    "\r\n",
    "train_df['sector'] = train_df.apply(\r\n",
    "    lambda x: making_sector_label(x['image']), axis=1\r\n",
    ")\r\n",
    "\r\n",
    "columns = train_df.columns.tolist()\r\n",
    "columns = columns[-1:] + columns[:-1]\r\n",
    "train_df = train_df[columns]\r\n",
    "train_df\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\r\n",
    "\r\n",
    "\r\n",
    "class SingleModelConfig:\r\n",
    "  def __init__(self,\r\n",
    "               input_size: List[int] = [384, 288],\r\n",
    "               epochs: int = 150,\r\n",
    "               batch_size: int = 6,\r\n",
    "               random_seed: int = 2021,\r\n",
    "               test_ratio: float = 0.15,\r\n",
    "               learning_rate: float = 1e-3,\r\n",
    "               num_joints: int = 24,\r\n",
    "               sigma: float = 3.0,\r\n",
    "               main_dir: str = main_dir,\r\n",
    "               extract: str = 'pose_fix',\r\n",
    "               loss_type: str = \"OHKMMSE\",\r\n",
    "               target_type: str = \"gaussian\",\r\n",
    "               post_processing: str = \"dark\",\r\n",
    "               mode: str = \"train\",\r\n",
    "               save_folder: str = 'single_hrdnet_w48_384x288_sectorBase_startify',\r\n",
    "               shift: bool = False,\r\n",
    "               udp: bool = False,\r\n",
    "               debug: bool = False,\r\n",
    "               startify: bool = False,\r\n",
    "               init_training: bool = False\r\n",
    "               ):\r\n",
    "\r\n",
    "    self.main_dir = main_dir\r\n",
    "    self.epochs = epochs\r\n",
    "    self.batch_size = batch_size\r\n",
    "    self.seed = random_seed\r\n",
    "    self.lr = learning_rate\r\n",
    "    self.startify = startify\r\n",
    "    self.test_ratio = test_ratio\r\n",
    "    self.image_size = np.array(input_size)\r\n",
    "    self.output_size = self.image_size//4\r\n",
    "    self.mode = mode\r\n",
    "    self.extract = extract\r\n",
    "    self.shift = shift\r\n",
    "    self.debug = debug\r\n",
    "    self.udp = udp\r\n",
    "    self.num_joints = num_joints\r\n",
    "    self.sigma = sigma\r\n",
    "    self.target_type = target_type\r\n",
    "    self.init_training = init_training\r\n",
    "    self.post_processing = post_processing\r\n",
    "    self.loss_type = loss_type\r\n",
    "\r\n",
    "    self.save_folder = os.path.join(main_dir, save_folder)\r\n",
    "    if not os.path.exists(self.save_folder):\r\n",
    "      os.makedirs(self.save_folder, exist_ok=True)\r\n",
    "\r\n",
    "    self.joints_name = {\r\n",
    "        0: 'nose', 1: 'left_eye', 2: 'right_eye', 3: 'left_ear', 4: 'right_ear',\r\n",
    "        5: 'left_shoulder', 6: 'right_shoulder', 7: 'left_elbow', 8: 'right_elbow',\r\n",
    "        9: 'left_wrist', 10: 'right_wrist', 11: 'left_hip', 12: 'right_hip',\r\n",
    "        13: 'left_knee', 14: 'right_knee', 15: 'left_ankle', 16: 'right_ankle',\r\n",
    "        17: 'neck', 18: 'left_palm', 19: 'right_palm', 20: 'back_spine', 21: 'waist_spine',\r\n",
    "        22: 'left_instep', 23: 'right_instep'\r\n",
    "    }\r\n",
    "\r\n",
    "    self.joint_pair = [\r\n",
    "        (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10),\r\n",
    "        (5, 7), (7, 9), (11, 13), (13, 15), (12, 14),\r\n",
    "        (14, 16), (5, 6), (15, 22), (16, 23), (11, 21),\r\n",
    "        (21, 12), (20, 21), (5, 20), (6, 20), (17, 6), (17, 5)\r\n",
    "    ]\r\n",
    "\r\n",
    "    self.flip_pair = [\r\n",
    "        (1, 2), (3, 4), (5, 6), (7, 8),\r\n",
    "        (9, 10), (11, 12), (13, 14), (15, 16),\r\n",
    "        (18, 19), (22, 23)\r\n",
    "    ]\r\n",
    "\r\n",
    "    self.joint_colors = {\r\n",
    "        k: tuple(map(int, np.random.randint(0, 255, 3))) for k in range(24)}\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = SingleModelConfig()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(cfg, image, keypoints, factor=None):\r\n",
    "    if keypoints.shape[-1] == 3:\r\n",
    "      keypoints = keypoints[:, :2].astype(np.int)\r\n",
    "\r\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n",
    "    colors = cfg.joint_colors\r\n",
    "\r\n",
    "    if factor is not None:\r\n",
    "      keypoints[:, 0] = keypoints[:, 0] * factor[0]\r\n",
    "      keypoints[:, 1] = keypoints[:, 1] * factor[1]\r\n",
    "\r\n",
    "    x1, y1 = int(min(keypoints[:, 0])), int(min(keypoints[:, 1]))\r\n",
    "    x2, y2 = int(max(keypoints[:, 0])), int(max(keypoints[:, 1]))\r\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 100, 91), thickness=3)\r\n",
    "\r\n",
    "    for i, keypoint in enumerate(keypoints):\r\n",
    "        cv2.circle(\r\n",
    "            image,\r\n",
    "            tuple(keypoint),\r\n",
    "            3, colors.get(i), thickness=3, lineType=cv2.FILLED)\r\n",
    "\r\n",
    "        cv2.putText(\r\n",
    "            image,\r\n",
    "            f'{i}: {cfg.joints_name[i]}',\r\n",
    "            tuple(keypoint),\r\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\r\n",
    "\r\n",
    "    for i, pair in enumerate(cfg.joint_pair):\r\n",
    "        cv2.line(\r\n",
    "            image,\r\n",
    "            tuple(keypoints[pair[0]]),\r\n",
    "            tuple(keypoints[pair[1]]),\r\n",
    "            colors.get(pair[0]), 3, lineType=cv2.LINE_AA)\r\n",
    "\r\n",
    "    fig, ax = plt.subplots(dpi=200)\r\n",
    "    ax.imshow(image)\r\n",
    "    ax.axis('off')\r\n",
    "    plt.show()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\r\n",
    "def get_affine_transform(center,\r\n",
    "                         scale,\r\n",
    "                         rot,\r\n",
    "                         output_size,\r\n",
    "                         shift=np.array([0, 0], dtype=np.float32),\r\n",
    "                         inv=0,):\r\n",
    "    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\r\n",
    "        print(scale)\r\n",
    "        scale = np.array([scale, scale])\r\n",
    "\r\n",
    "    src_w = scale[0]\r\n",
    "    dst_w = output_size[0]\r\n",
    "    dst_h = output_size[1]\r\n",
    "\r\n",
    "    rot_rad = np.pi * rot / 180\r\n",
    "    src_dir = get_dir([0, src_w * -0.5], rot_rad)\r\n",
    "    dst_dir = np.array([0, dst_w * -0.5], np.float32)\r\n",
    "\r\n",
    "    src = np.zeros((3, 2), dtype=np.float32)\r\n",
    "    dst = np.zeros((3, 2), dtype=np.float32)\r\n",
    "    src[0, :] = center + scale * shift\r\n",
    "    src[1, :] = center + src_dir + scale * shift\r\n",
    "    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\r\n",
    "    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\r\n",
    "\r\n",
    "    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\r\n",
    "    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\r\n",
    "\r\n",
    "    if inv:\r\n",
    "        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\r\n",
    "    else:\r\n",
    "        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\r\n",
    "\r\n",
    "    return trans\r\n",
    "\r\n",
    "\r\n",
    "# 실제 어파인 변환 수행하는 부분\r\n",
    "# t는 위의 get_affine_transform 함수를 통해 trans 매트릭스 구한다.\r\n",
    "def affine_transform(pt, t):\r\n",
    "    new_pt = np.array([pt[0], pt[1], 1.]).T\r\n",
    "    new_pt = np.dot(t, new_pt)\r\n",
    "    return new_pt[:2]\r\n",
    "\r\n",
    "\r\n",
    "# TODO: 뭐하는 함수지?\r\n",
    "def get_dir(src_point, rot_rad):\r\n",
    "    \"\"\" \r\n",
    "        Transformation Matrix \r\n",
    "        x = x * cosΘ - y * sinΘ\r\n",
    "        y = x * cosΘ + y * sinΘ\r\n",
    "        [ cosΘ   sinΘ   0]\r\n",
    "        [ -sinΘ  cosΘ   0]\r\n",
    "        [  0       0    0]\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\r\n",
    "\r\n",
    "    src_result = [0, 0]\r\n",
    "    src_result[0] = src_point[0] * cs - src_point[1] * sn\r\n",
    "    src_result[1] = src_point[0] * sn + src_point[1] * cs\r\n",
    "\r\n",
    "    return src_result\r\n",
    "\r\n",
    "\r\n",
    "def get_3rd_point(a, b):\r\n",
    "    direct = a - b\r\n",
    "    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\r\n",
    "\r\n",
    "\r\n",
    "def shift_images(image, keypoints, max_v=5, max_h=5):\r\n",
    "    tmp_keypoints = np.ones_like(keypoints)\r\n",
    "    shift_v = np.random.randint(low=-max_v, high=max_v)\r\n",
    "    shift_h = np.random.randint(low=-max_h, high=max_h)\r\n",
    "\r\n",
    "    m = np.float32([\r\n",
    "        [1, 0, shift_h],\r\n",
    "        [0, 1, shift_v],\r\n",
    "    ])\r\n",
    "\r\n",
    "    rows, cols = image.shape[:-1]\r\n",
    "    image = cv2.warpAffine(image, m, (cols, rows))\r\n",
    "\r\n",
    "    # tmp_keypoints[:, 0] = keypoints[:, 0] + shift_v/cols\r\n",
    "    # tmp_keypoints[:, 1] = keypoints[:, 1] + shift_h/rows\r\n",
    "    for j in range(len(keypoints)):\r\n",
    "        keypoints[j, :2] = affine_transform(keypoints[j, :2], m)\r\n",
    "\r\n",
    "    return image, keypoints\r\n",
    "## UDP\r\n",
    "## https://github.com/HuangJunJie2017/UDP-Pose/blob/master/deep-high-resolution-net.pytorch/lib/dataset/JointsDataset.py\r\n",
    "\r\n",
    "\r\n",
    "def get_warpmatrix(theta, size_input, size_dst, size_target):\r\n",
    "    '''\r\n",
    "    :param theta: angle\r\n",
    "    :param size_input:[w,h]\r\n",
    "    :param size_dst: [w,h]\r\n",
    "    :param size_target: [w,h]/200.0\r\n",
    "    :return:\r\n",
    "    '''\r\n",
    "    size_target = size_target * 200.0\r\n",
    "    theta = theta / 180.0 * math.pi\r\n",
    "    matrix = np.zeros((2, 3), dtype=np.float32)\r\n",
    "    scale_x = size_target[0]/size_dst[0]\r\n",
    "    scale_y = size_target[1]/size_dst[1]\r\n",
    "    matrix[0, 0] = math.cos(theta) * scale_x\r\n",
    "    matrix[0, 1] = math.sin(theta) * scale_y\r\n",
    "    matrix[0, 2] = -0.5 * size_target[0] * \\\r\n",
    "        math.cos(theta) - 0.5 * size_target[1] * \\\r\n",
    "        math.sin(theta) + 0.5 * size_input[0]\r\n",
    "    matrix[1, 0] = -math.sin(theta) * scale_x\r\n",
    "    matrix[1, 1] = math.cos(theta) * scale_y\r\n",
    "    matrix[1, 2] = 0.5*size_target[0] * \\\r\n",
    "        math.sin(theta)-0.5*size_target[1]*math.cos(theta)+0.5*size_input[1]\r\n",
    "    return matrix\r\n",
    "\r\n",
    "\r\n",
    "def rotate_points(src_points, angle, c, dst_img_shape, size_target, do_clip=True):\r\n",
    "    # src_points: (num_points, 2)\r\n",
    "    # img_shape: [h, w, c]\r\n",
    "    size_target = size_target * 200.0\r\n",
    "    src_img_center = c\r\n",
    "    scale_x = (dst_img_shape[0]-1.0)/size_target[0]\r\n",
    "    scale_y = (dst_img_shape[1]-1.0)/size_target[1]\r\n",
    "    radian = angle / 180.0 * math.pi\r\n",
    "    radian_sin = -math.sin(radian)\r\n",
    "    radian_cos = math.cos(radian)\r\n",
    "    dst_points = np.zeros(src_points.shape, dtype=src_points.dtype)\r\n",
    "    src_x = src_points[:, 0] - src_img_center[0]\r\n",
    "    src_y = src_points[:, 1] - src_img_center[1]\r\n",
    "    dst_points[:, 0] = radian_cos * src_x + radian_sin * src_y\r\n",
    "    dst_points[:, 1] = -radian_sin * src_x + radian_cos * src_y\r\n",
    "    dst_points[:, 0] += size_target[0]*0.5\r\n",
    "    dst_points[:, 1] += size_target[1]*0.5\r\n",
    "    dst_points[:, 0] *= scale_x\r\n",
    "    dst_points[:, 1] *= scale_y\r\n",
    "    if do_clip:\r\n",
    "        # if dst_points[:, 0] < 0 or dst_points[:, 0] >= dst_img_shape[1]: dst_points[:, 2] = 0\r\n",
    "        # if dst_points[:, 1] < 0 or dst_points[:, 1] >= dst_img_shape[0]: dst_points[:, 2] = 0\r\n",
    "\r\n",
    "        dst_points[:, 0] = np.clip(dst_points[:, 0], 0, dst_img_shape[1] - 1)\r\n",
    "        dst_points[:, 1] = np.clip(dst_points[:, 1], 0, dst_img_shape[0] - 1)\r\n",
    "    return dst_points\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaconKeypointsDataset(Dataset):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        image_dir,\r\n",
    "        label_df,\r\n",
    "        transforms=None,\r\n",
    "        cfg=cfg\r\n",
    "    ) -> None:\r\n",
    "        self.image_dir = image_dir\r\n",
    "        self.df = label_df\r\n",
    "        self.transforms = transforms\r\n",
    "\r\n",
    "        self.udp = cfg.udp\r\n",
    "        self.mode = cfg.mode\r\n",
    "        self.debug = cfg.debug\r\n",
    "        self.shift = cfg.shift\r\n",
    "        self.num_joints = cfg.num_joints\r\n",
    "        self.flip_pairs = cfg.flip_pair\r\n",
    "        self.image_size = cfg.image_size\r\n",
    "        self.heatmap_size = cfg.output_size\r\n",
    "        self.sigma = cfg.sigma\r\n",
    "        self.target_type = cfg.target_type\r\n",
    "\r\n",
    "    def __len__(self) -> int:\r\n",
    "        return self.df.shape[0]\r\n",
    "\r\n",
    "    def __getitem__(self, index: int):\r\n",
    "        image_id = self.df.iloc[index, 0]\r\n",
    "        labels = np.array([1])\r\n",
    "        keypoints = self.df.iloc[index,\r\n",
    "                                 1:].values.reshape(-1, 2).astype(np.float32)\r\n",
    "        keypoints = np.concatenate([keypoints,  np.ones((24, 1))], axis=1)\r\n",
    "\r\n",
    "        # define bbox\r\n",
    "        xmin = np.min(keypoints[:, 0])\r\n",
    "        xmax = np.max(keypoints[:, 0])\r\n",
    "        width = xmax - xmin if xmax > xmin else 20\r\n",
    "        center = (xmin + xmax)/2.\r\n",
    "        xmin = int(center - width/2.*1.2)\r\n",
    "        xmax = int(center + width/2.*1.2)\r\n",
    "\r\n",
    "        ymin = np.min(keypoints[:, 1])\r\n",
    "        ymax = np.max(keypoints[:, 1])\r\n",
    "        height = ymax - ymin if ymax > ymin else 20\r\n",
    "        center = (ymin + ymax)/2.\r\n",
    "        ymin = int(center - height/2.*1.2)\r\n",
    "        ymax = int(center + height/2.*1.2)\r\n",
    "\r\n",
    "        x, y, w, h = xmin, ymin, xmax-xmin, ymax-ymin\r\n",
    "        aspect_ratio = self.image_size[1] / self.image_size[0]\r\n",
    "        centre = np.array([x+w*.5, y+h*.5])\r\n",
    "        if w > aspect_ratio * h:\r\n",
    "            h = w * 1.0 / aspect_ratio\r\n",
    "        elif w < aspect_ratio * h:\r\n",
    "            w = h * aspect_ratio\r\n",
    "\r\n",
    "        # todo: check why divide by pixel_std\r\n",
    "        use_udp = np.random.rand() < 0.5 and self.udp\r\n",
    "\r\n",
    "        if use_udp:\r\n",
    "          scale = np.array(\r\n",
    "              [\r\n",
    "                  w * 1.0 / self.pixel_std,\r\n",
    "                  h * 1.0 / self.pixel_std\r\n",
    "              ],\r\n",
    "              dtype=np.float32\r\n",
    "          )\r\n",
    "          scale = scale * 1.5\r\n",
    "        else:\r\n",
    "          scale = np.array([w, h]) * 1.25\r\n",
    "        rotation = 0\r\n",
    "\r\n",
    "        image = cv2.imread(os.path.join(\r\n",
    "            self.image_dir, image_id), cv2.COLOR_BGR2RGB)\r\n",
    "        # if it's train mode\r\n",
    "        if self.mode == 'train':\r\n",
    "            scale_factor = 0.3\r\n",
    "            rotate_factor = 45\r\n",
    "            scale = scale * \\\r\n",
    "                np.clip(np.random.randn()*scale_factor +\r\n",
    "                        1, 1-scale_factor, 1+scale_factor)\r\n",
    "            rotation = np.clip(np.random.randn()*rotate_factor, -rotate_factor*2,\r\n",
    "                               rotate_factor*2) if random.random() <= 0.5 else 0\r\n",
    "\r\n",
    "            # lr flipping\r\n",
    "            if np.random.random() <= 0.5:\r\n",
    "              image = np.flip(image, 1)\r\n",
    "              centre[0] = image.shape[1] - 1 - centre[0]\r\n",
    "\r\n",
    "              keypoints[:, 0] = image.shape[1] - 1 - keypoints[:, 0]\r\n",
    "              for (q, w) in self.flip_pairs:\r\n",
    "                  keypoints_q, keypoints_w = keypoints[q, :].copy(\r\n",
    "                  ), keypoints[w, :].copy()\r\n",
    "                  keypoints[w, :], keypoints[q, :] = keypoints_q, keypoints_w\r\n",
    "\r\n",
    "            # if self.transforms is not None:\r\n",
    "            #   image = self.transforms(image=image)['image']\r\n",
    "\r\n",
    "        if use_udp:\r\n",
    "          trans = get_warpmatrix(rotation, centre*2.0,\r\n",
    "                                 self.image_size-1.0, scale)\r\n",
    "          cropped_image = cv2.warpAffine(image, trans, (int(self.image_size[1]), int(\r\n",
    "              self.image_size[0])), flags=cv2.WARP_INVERSE_MAP | cv2.INTER_LINEAR)\r\n",
    "          keypoints[:, 0:2] = rotate_points(\r\n",
    "              keypoints[:, 0:2], rotation, centre, self.image_size, scale, True)\r\n",
    "        else:\r\n",
    "          trans = get_affine_transform(\r\n",
    "              centre, scale, rotation, (self.image_size[1], self.image_size[0]))\r\n",
    "          cropped_image = cv2.warpAffine(\r\n",
    "              image, trans, (self.image_size[1], self.image_size[0]), flags=cv2.INTER_LINEAR)\r\n",
    "          for j in range(self.num_joints):\r\n",
    "              if keypoints[j, 2] > 0:\r\n",
    "                  keypoints[j, :2] = affine_transform(keypoints[j, :2], trans)\r\n",
    "                  keypoints[j, 2] *= ((keypoints[j, 0] >= 0) & (keypoints[j, 0] < self.image_size[1])\r\n",
    "                                      & (keypoints[j, 1] >= 0) & (keypoints[j, 1] < self.image_size[0]))\r\n",
    "\r\n",
    "        target, target_weight = self.generate_target(\r\n",
    "            keypoints[:, :2], keypoints[:, 2])\r\n",
    "        target = torch.from_numpy(target)\r\n",
    "        target_weight = torch.from_numpy(target_weight)\r\n",
    "\r\n",
    "        if self.transforms is not None:\r\n",
    "            cropped_image = self.transforms(image=cropped_image)['image']\r\n",
    "\r\n",
    "        # random horizontal & vertical shifting\r\n",
    "        if self.mode == 'train' and self.shift and np.random.random() <= 0.5:\r\n",
    "            cropped_image, keypoints = shift_images(cropped_image, keypoints)\r\n",
    "\r\n",
    "        if self.debug:\r\n",
    "          show_image(cropped_image, keypoints)\r\n",
    "\r\n",
    "          target_heatmap = self.render_gaussian_heatmap(\r\n",
    "              keypoints[:, :2], output_shape=self.heatmap_size)\r\n",
    "          visualize_heatmap = target_heatmap  # * 255.\r\n",
    "          visualize_heatmap = visualize_heatmap.astype('uint8')[0]\r\n",
    "          visualize_heatmap = np.max(visualize_heatmap, axis=2)\r\n",
    "          visualize_heatmap = cv2.applyColorMap(\r\n",
    "              visualize_heatmap, cv2.COLORMAP_JET)\r\n",
    "          fig, ax = plt.subplots(dpi=200)\r\n",
    "          ax.imshow(visualize_heatmap)\r\n",
    "          ax.axis('off')\r\n",
    "          plt.show()\r\n",
    "\r\n",
    "        sample = {\r\n",
    "            'image': torch.from_numpy(cropped_image).float().permute(2, 0, 1),\r\n",
    "            'keypoints': torch.from_numpy(keypoints).float(),\r\n",
    "            'target': target,\r\n",
    "            'target_weight': target_weight\r\n",
    "        }\r\n",
    "        return sample\r\n",
    "\r\n",
    "    def render_gaussian_heatmap(self, coord, valid=None):\r\n",
    "        \"\"\"\r\n",
    "            Render gaussian heatmap\r\n",
    "            \r\n",
    "            [Args]\r\n",
    "                [coord]: coords for gaussian rendering\r\n",
    "                \r\n",
    "                [output_shape]: output heatmap shape\r\n",
    "                \r\n",
    "                [sigma]: gaussian sigma\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        x = [i for i in range(self.heatmap_size[1])]\r\n",
    "        y = [i for i in range(self.heatmap_size[0])]\r\n",
    "        xx, yy = np.meshgrid(x, y)\r\n",
    "\r\n",
    "        if isinstance(coord, np.ndarray):\r\n",
    "          xx = np.reshape(xx, (1, *self.heatmap_size, 1))\r\n",
    "          yy = np.reshape(yy, (1, *self.heatmap_size, 1))\r\n",
    "\r\n",
    "          x = np.reshape(coord[:, 0], [-1, 1, 1, self.num_joints]\r\n",
    "                         ) / self.image_size[1] * self.heatmap_size[1]\r\n",
    "          y = np.reshape(coord[:, 1], [-1, 1, 1, self.num_joints]\r\n",
    "                         ) / self.image_size[0] * self.heatmap_size[0]\r\n",
    "\r\n",
    "          heatmap = np.exp(-(((xx-x)/self.sigma)**2) /\r\n",
    "                           2.0 - (((yy-y)/self.sigma)**2)/2.0)\r\n",
    "          return heatmap * 255.\r\n",
    "          # return heatmap\r\n",
    "        elif isinstance(coord, torch.Tensor):\r\n",
    "          xx = torch.from_numpy(xx)\r\n",
    "          yy = torch.from_numpy(yy)\r\n",
    "          xx = torch.reshape(xx.float(), (1, *self.heatmap_size, 1))\r\n",
    "          yy = torch.reshape(yy.float(), (1, *self.heatmap_size, 1))\r\n",
    "\r\n",
    "          x = torch.reshape(coord[:, :, 0], [-1, 1, 1, self.num_joints]\r\n",
    "                            ) / self.image_size[1] * self.heatmap_size[1]\r\n",
    "          y = torch.reshape(coord[:, :, 1], [-1, 1, 1, self.num_joints]\r\n",
    "                            ) / self.image_size[0] * self.heatmap_size[0]\r\n",
    "\r\n",
    "          heatmap = torch.exp(-(((xx-x)/torch.tensor(self.sigma, dtype=torch.float))**2) /\r\n",
    "                              2.0 - (((yy-y)/torch.tensor(self.sigma, dtype=torch.float))**2)/2.0)\r\n",
    "          if valid is not None:\r\n",
    "              valid_mask = torch.reshape(valid, [-1, 1, 1, self.num_joints])\r\n",
    "              heatmap = heatmap * valid_mask\r\n",
    "          return (heatmap*255.).permute(0, 3, 1, 2)\r\n",
    "          # return heatmap.permute(0,3,1,2)\r\n",
    "\r\n",
    "    def generate_target(self, joints, joints_vis):\r\n",
    "        '''\r\n",
    "        :param joints:  [num_joints, 3]\r\n",
    "        :param joints_vis: [num_joints, 3]\r\n",
    "        :return: target, target_weight(1: visible, 0: invisible)\r\n",
    "        '''\r\n",
    "        target_weight = np.ones((self.num_joints, 1), dtype=np.float32)\r\n",
    "        target_weight[:, 0] = joints_vis\r\n",
    "\r\n",
    "        assert self.target_type == 'gaussian', \\\r\n",
    "            'Only support gaussian map now!'\r\n",
    "\r\n",
    "        if self.target_type == 'gaussian':\r\n",
    "            target = np.zeros((self.num_joints,\r\n",
    "                               self.heatmap_size[0],\r\n",
    "                               self.heatmap_size[1]),\r\n",
    "                              dtype=np.float32)\r\n",
    "            tmp_size = self.sigma * 3\r\n",
    "\r\n",
    "            for joint_id in range(self.num_joints):\r\n",
    "                feat_stride = self.image_size / self.heatmap_size\r\n",
    "                mu_x = int(joints[joint_id][0] / feat_stride[0] + 0.5)\r\n",
    "                mu_y = int(joints[joint_id][1] / feat_stride[1] + 0.5)\r\n",
    "                # Check that any part of the gaussian is in-bounds\r\n",
    "                ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\r\n",
    "                br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\r\n",
    "                if ul[0] >= self.heatmap_size[0] or ul[1] >= self.heatmap_size[1] \\\r\n",
    "                        or br[0] < 0 or br[1] < 0:\r\n",
    "                    # If not, just return the image as is\r\n",
    "                    target_weight[joint_id] = 0\r\n",
    "                    continue\r\n",
    "\r\n",
    "                # # Generate gaussian\r\n",
    "                size = 2 * tmp_size + 1\r\n",
    "                x = np.arange(0, size, 1, np.float32)\r\n",
    "                y = x[:, np.newaxis]\r\n",
    "                x0 = y0 = size // 2\r\n",
    "                # The gaussian is not normalized, we want the center value to equal 1\r\n",
    "                g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) /\r\n",
    "                           (2 * self.sigma ** 2))\r\n",
    "\r\n",
    "                # Usable gaussian range\r\n",
    "                g_x = max(0, -ul[0]), min(br[0], self.heatmap_size[1]) - ul[0]\r\n",
    "                g_y = max(0, -ul[1]), min(br[1], self.heatmap_size[0]) - ul[1]\r\n",
    "                # Image range\r\n",
    "                img_x = max(0, ul[0]), min(br[0], self.heatmap_size[1])\r\n",
    "                img_y = max(0, ul[1]), min(br[1], self.heatmap_size[0])\r\n",
    "\r\n",
    "                v = target_weight[joint_id]\r\n",
    "                if v > 0.5:\r\n",
    "                    # plt.imshow(g[g_y[0]:g_y[1], g_x[0]:g_x[1]])\r\n",
    "                    # plt.show()\r\n",
    "\r\n",
    "                    target[joint_id][img_y[0]:img_y[1], img_x[0]:img_x[1]] = \\\r\n",
    "                        g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\r\n",
    "\r\n",
    "        # if self.use_different_joints_weight:\r\n",
    "        #     target_weight = np.multiply(target_weight, self.joints_weight)\r\n",
    "\r\n",
    "        return target * 255., target_weight\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/lib/core/loss.py#L15:7\r\n",
    "class JointsRMSELoss(nn.Module):\r\n",
    "    def __init__(self, use_target_weight=False):\r\n",
    "        super(JointsRMSELoss, self).__init__()\r\n",
    "        self.criterion = nn.MSELoss(reduction='none')\r\n",
    "\r\n",
    "    def forward(self, pred, target):\r\n",
    "        target_coord = target[:, :, :2]\r\n",
    "        valid_mask = target[:, :, 2].unsqueeze(-1)\r\n",
    "        loss = self.criterion(pred, target_coord) * valid_mask\r\n",
    "        loss = torch.sqrt(torch.mean(torch.mean(loss, dim=-1)))\r\n",
    "        return loss\r\n",
    "\r\n",
    "\r\n",
    "class JointsMSELoss(nn.Module):\r\n",
    "    def __init__(self, use_target_weight=True):\r\n",
    "        super(JointsMSELoss, self).__init__()\r\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\r\n",
    "        self.use_target_weight = use_target_weight\r\n",
    "\r\n",
    "    def forward(self, output, target, target_weight):\r\n",
    "        batch_size = output.size(0)\r\n",
    "        num_joints = output.size(1)\r\n",
    "        heatmaps_pred = output.reshape(\r\n",
    "            (batch_size, num_joints, -1)).split(1, 1)\r\n",
    "        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\r\n",
    "\r\n",
    "        loss = 0\r\n",
    "\r\n",
    "        for idx in range(num_joints):\r\n",
    "            heatmap_pred = heatmaps_pred[idx].squeeze()\r\n",
    "            heatmap_gt = heatmaps_gt[idx].squeeze()\r\n",
    "\r\n",
    "            if self.use_target_weight:\r\n",
    "                loss += 0.5 * self.criterion(\r\n",
    "                    heatmap_pred.mul(target_weight[:, idx]),\r\n",
    "                    heatmap_gt.mul(target_weight[:, idx])\r\n",
    "                )\r\n",
    "            else:\r\n",
    "                loss += 0.5 * self.criterion(heatmap_pred, heatmap_gt)\r\n",
    "\r\n",
    "        return loss / num_joints\r\n",
    "\r\n",
    "\r\n",
    "class JointsOHKMMSELoss(nn.Module):\r\n",
    "    def __init__(self, use_target_weight=True, topk=8):\r\n",
    "        super(JointsOHKMMSELoss, self).__init__()\r\n",
    "        self.criterion = nn.MSELoss(reduction='none')\r\n",
    "        self.use_target_weight = use_target_weight\r\n",
    "        self.topk = topk\r\n",
    "\r\n",
    "    def ohkm(self, loss):\r\n",
    "        ohkm_loss = 0.\r\n",
    "        for i in range(loss.size()[0]):\r\n",
    "            sub_loss = loss[i]\r\n",
    "            topk_val, topk_idx = torch.topk(\r\n",
    "                sub_loss, k=self.topk, dim=0, sorted=False\r\n",
    "            )\r\n",
    "            tmp_loss = torch.gather(sub_loss, 0, topk_idx)\r\n",
    "            ohkm_loss += torch.sum(tmp_loss) / self.topk\r\n",
    "        ohkm_loss /= loss.size()[0]\r\n",
    "        return ohkm_loss\r\n",
    "\r\n",
    "    def forward(self, output, target, target_weight):\r\n",
    "        batch_size = output.size(0)\r\n",
    "        num_joints = output.size(1)\r\n",
    "        heatmaps_pred = output.reshape(\r\n",
    "            (batch_size, num_joints, -1)).split(1, 1)\r\n",
    "        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\r\n",
    "\r\n",
    "        loss = []\r\n",
    "        for idx in range(num_joints):\r\n",
    "            heatmap_pred = heatmaps_pred[idx].squeeze()\r\n",
    "            heatmap_gt = heatmaps_gt[idx].squeeze()\r\n",
    "            if self.use_target_weight:\r\n",
    "                loss.append(float(0.5 * self.criterion(\r\n",
    "                    heatmap_pred.mul(target_weight[:, idx]),\r\n",
    "                    heatmap_gt.mul(target_weight[:, idx])\r\n",
    "                )))\r\n",
    "            else:\r\n",
    "                loss.append(\r\n",
    "                    float(0.5 * self.criterion(heatmap_pred, heatmap_gt))\r\n",
    "                )\r\n",
    "\r\n",
    "        loss = [l.mean(dim=1).unsqueeze(dim=1) for l in loss]\r\n",
    "        loss = torch.cat(loss, dim=1)\r\n",
    "\r\n",
    "        return self.ohkm(loss)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/lib/models/pose_hrnet.py\r\n",
    "# HRNET\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "\r\n",
    "BN_MOMENTUM = 0.1\r\n",
    "\r\n",
    "\r\n",
    "def conv3x3(in_planes, out_planes, stride=1):\r\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\r\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n",
    "                     padding=1, bias=False)\r\n",
    "\r\n",
    "\r\n",
    "class BasicBlock(nn.Module):\r\n",
    "    expansion = 1\r\n",
    "\r\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n",
    "        super(BasicBlock, self).__init__()\r\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\r\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\r\n",
    "        self.relu = nn.ReLU(inplace=True)\r\n",
    "        self.conv2 = conv3x3(planes, planes)\r\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\r\n",
    "        self.downsample = downsample\r\n",
    "        self.stride = stride\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        residual = x\r\n",
    "\r\n",
    "        out = self.conv1(x)\r\n",
    "        out = self.bn1(out)\r\n",
    "        out = self.relu(out)\r\n",
    "\r\n",
    "        out = self.conv2(out)\r\n",
    "        out = self.bn2(out)\r\n",
    "\r\n",
    "        if self.downsample is not None:\r\n",
    "            residual = self.downsample(x)\r\n",
    "\r\n",
    "        out += residual\r\n",
    "        out = self.relu(out)\r\n",
    "\r\n",
    "        return out\r\n",
    "\r\n",
    "\r\n",
    "class Bottleneck(nn.Module):\r\n",
    "    expansion = 4\r\n",
    "\r\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n",
    "        super(Bottleneck, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\r\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n",
    "                               padding=1, bias=False)\r\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\r\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\r\n",
    "                               bias=False)\r\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\r\n",
    "                                  momentum=BN_MOMENTUM)\r\n",
    "        self.relu = nn.ReLU(inplace=True)\r\n",
    "        self.downsample = downsample\r\n",
    "        self.stride = stride\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        residual = x\r\n",
    "\r\n",
    "        out = self.conv1(x)\r\n",
    "        out = self.bn1(out)\r\n",
    "        out = self.relu(out)\r\n",
    "\r\n",
    "        out = self.conv2(out)\r\n",
    "        out = self.bn2(out)\r\n",
    "        out = self.relu(out)\r\n",
    "\r\n",
    "        out = self.conv3(out)\r\n",
    "        out = self.bn3(out)\r\n",
    "\r\n",
    "        if self.downsample is not None:\r\n",
    "            residual = self.downsample(x)\r\n",
    "\r\n",
    "        out += residual\r\n",
    "        out = self.relu(out)\r\n",
    "\r\n",
    "        return out\r\n",
    "\r\n",
    "\r\n",
    "class HighResolutionModule(nn.Module):\r\n",
    "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\r\n",
    "                 num_channels, fuse_method, multi_scale_output=True):\r\n",
    "        super(HighResolutionModule, self).__init__()\r\n",
    "        self._check_branches(\r\n",
    "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\r\n",
    "\r\n",
    "        self.num_inchannels = num_inchannels\r\n",
    "        self.fuse_method = fuse_method\r\n",
    "        self.num_branches = num_branches\r\n",
    "\r\n",
    "        self.multi_scale_output = multi_scale_output\r\n",
    "\r\n",
    "        self.branches = self._make_branches(\r\n",
    "            num_branches, blocks, num_blocks, num_channels)\r\n",
    "        self.fuse_layers = self._make_fuse_layers()\r\n",
    "        self.relu = nn.ReLU(True)\r\n",
    "\r\n",
    "    def _check_branches(self, num_branches, blocks, num_blocks,\r\n",
    "                        num_inchannels, num_channels):\r\n",
    "        if num_branches != len(num_blocks):\r\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\r\n",
    "                num_branches, len(num_blocks))\r\n",
    "            logger.error(error_msg)\r\n",
    "            raise ValueError(error_msg)\r\n",
    "\r\n",
    "        if num_branches != len(num_channels):\r\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\r\n",
    "                num_branches, len(num_channels))\r\n",
    "            logger.error(error_msg)\r\n",
    "            raise ValueError(error_msg)\r\n",
    "\r\n",
    "        if num_branches != len(num_inchannels):\r\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\r\n",
    "                num_branches, len(num_inchannels))\r\n",
    "            logger.error(error_msg)\r\n",
    "            raise ValueError(error_msg)\r\n",
    "\r\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\r\n",
    "                         stride=1):\r\n",
    "        downsample = None\r\n",
    "        if stride != 1 or \\\r\n",
    "           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\r\n",
    "            downsample = nn.Sequential(\r\n",
    "                nn.Conv2d(\r\n",
    "                    self.num_inchannels[branch_index],\r\n",
    "                    num_channels[branch_index] * block.expansion,\r\n",
    "                    kernel_size=1, stride=stride, bias=False\r\n",
    "                ),\r\n",
    "                nn.BatchNorm2d(\r\n",
    "                    num_channels[branch_index] * block.expansion,\r\n",
    "                    momentum=BN_MOMENTUM\r\n",
    "                ),\r\n",
    "            )\r\n",
    "\r\n",
    "        layers = []\r\n",
    "        layers.append(\r\n",
    "            block(\r\n",
    "                self.num_inchannels[branch_index],\r\n",
    "                num_channels[branch_index],\r\n",
    "                stride,\r\n",
    "                downsample\r\n",
    "            )\r\n",
    "        )\r\n",
    "        self.num_inchannels[branch_index] = \\\r\n",
    "            num_channels[branch_index] * block.expansion\r\n",
    "        for i in range(1, num_blocks[branch_index]):\r\n",
    "            layers.append(\r\n",
    "                block(\r\n",
    "                    self.num_inchannels[branch_index],\r\n",
    "                    num_channels[branch_index]\r\n",
    "                )\r\n",
    "            )\r\n",
    "\r\n",
    "        return nn.Sequential(*layers)\r\n",
    "\r\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\r\n",
    "        branches = []\r\n",
    "\r\n",
    "        for i in range(num_branches):\r\n",
    "            branches.append(\r\n",
    "                self._make_one_branch(i, block, num_blocks, num_channels)\r\n",
    "            )\r\n",
    "\r\n",
    "        return nn.ModuleList(branches)\r\n",
    "\r\n",
    "    def _make_fuse_layers(self):\r\n",
    "        if self.num_branches == 1:\r\n",
    "            return None\r\n",
    "\r\n",
    "        num_branches = self.num_branches\r\n",
    "        num_inchannels = self.num_inchannels\r\n",
    "        fuse_layers = []\r\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\r\n",
    "            fuse_layer = []\r\n",
    "            for j in range(num_branches):\r\n",
    "                if j > i:\r\n",
    "                    fuse_layer.append(\r\n",
    "                        nn.Sequential(\r\n",
    "                            nn.Conv2d(\r\n",
    "                                num_inchannels[j],\r\n",
    "                                num_inchannels[i],\r\n",
    "                                1, 1, 0, bias=False\r\n",
    "                            ),\r\n",
    "                            nn.BatchNorm2d(num_inchannels[i]),\r\n",
    "                            nn.Upsample(scale_factor=2**(j-i), mode='nearest')\r\n",
    "                        )\r\n",
    "                    )\r\n",
    "                elif j == i:\r\n",
    "                    fuse_layer.append(None)\r\n",
    "                else:\r\n",
    "                    conv3x3s = []\r\n",
    "                    for k in range(i-j):\r\n",
    "                        if k == i - j - 1:\r\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\r\n",
    "                            conv3x3s.append(\r\n",
    "                                nn.Sequential(\r\n",
    "                                    nn.Conv2d(\r\n",
    "                                        num_inchannels[j],\r\n",
    "                                        num_outchannels_conv3x3,\r\n",
    "                                        3, 2, 1, bias=False\r\n",
    "                                    ),\r\n",
    "                                    nn.BatchNorm2d(num_outchannels_conv3x3)\r\n",
    "                                )\r\n",
    "                            )\r\n",
    "                        else:\r\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\r\n",
    "                            conv3x3s.append(\r\n",
    "                                nn.Sequential(\r\n",
    "                                    nn.Conv2d(\r\n",
    "                                        num_inchannels[j],\r\n",
    "                                        num_outchannels_conv3x3,\r\n",
    "                                        3, 2, 1, bias=False\r\n",
    "                                    ),\r\n",
    "                                    nn.BatchNorm2d(num_outchannels_conv3x3),\r\n",
    "                                    nn.ReLU(True)\r\n",
    "                                )\r\n",
    "                            )\r\n",
    "                    fuse_layer.append(nn.Sequential(*conv3x3s))\r\n",
    "            fuse_layers.append(nn.ModuleList(fuse_layer))\r\n",
    "\r\n",
    "        return nn.ModuleList(fuse_layers)\r\n",
    "\r\n",
    "    def get_num_inchannels(self):\r\n",
    "        return self.num_inchannels\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        if self.num_branches == 1:\r\n",
    "            return [self.branches[0](x[0])]\r\n",
    "\r\n",
    "        for i in range(self.num_branches):\r\n",
    "            x[i] = self.branches[i](x[i])\r\n",
    "\r\n",
    "        x_fuse = []\r\n",
    "\r\n",
    "        for i in range(len(self.fuse_layers)):\r\n",
    "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\r\n",
    "            for j in range(1, self.num_branches):\r\n",
    "                if i == j:\r\n",
    "                    y = y + x[j]\r\n",
    "                else:\r\n",
    "                    y = y + self.fuse_layers[i][j](x[j])\r\n",
    "            x_fuse.append(self.relu(y))\r\n",
    "\r\n",
    "        return x_fuse\r\n",
    "\r\n",
    "\r\n",
    "blocks_dict = {\r\n",
    "    'BASIC': BasicBlock,\r\n",
    "    'BOTTLENECK': Bottleneck\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "class PoseHighResolutionNet(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, cfg, **kwargs):\r\n",
    "        self.inplanes = 64\r\n",
    "        extra = cfg['MODEL']['EXTRA']\r\n",
    "        super(PoseHighResolutionNet, self).__init__()\r\n",
    "\r\n",
    "        # stem net\r\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\r\n",
    "                               bias=False)\r\n",
    "        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\r\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\r\n",
    "                               bias=False)\r\n",
    "        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\r\n",
    "        self.relu = nn.ReLU(inplace=True)\r\n",
    "        self.layer1 = self._make_layer(Bottleneck, 64, 4)\r\n",
    "\r\n",
    "        self.stage2_cfg = extra['STAGE2']\r\n",
    "        num_channels = self.stage2_cfg['NUM_CHANNELS']\r\n",
    "        block = blocks_dict['BASIC']\r\n",
    "        num_channels = [\r\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))\r\n",
    "        ]\r\n",
    "        self.transition1 = self._make_transition_layer([256], num_channels)\r\n",
    "        self.stage2, pre_stage_channels = self._make_stage(\r\n",
    "            self.stage2_cfg, num_channels)\r\n",
    "\r\n",
    "        self.stage3_cfg = extra['STAGE3']\r\n",
    "        num_channels = self.stage3_cfg['NUM_CHANNELS']\r\n",
    "        block = blocks_dict[self.stage3_cfg['BLOCK']]\r\n",
    "        num_channels = [\r\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))\r\n",
    "        ]\r\n",
    "        self.transition2 = self._make_transition_layer(\r\n",
    "            pre_stage_channels, num_channels)\r\n",
    "        self.stage3, pre_stage_channels = self._make_stage(\r\n",
    "            self.stage3_cfg, num_channels)\r\n",
    "\r\n",
    "        self.stage4_cfg = extra['STAGE4']\r\n",
    "        num_channels = self.stage4_cfg['NUM_CHANNELS']\r\n",
    "        block = blocks_dict[self.stage4_cfg['BLOCK']]\r\n",
    "        num_channels = [\r\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))\r\n",
    "        ]\r\n",
    "        self.transition3 = self._make_transition_layer(\r\n",
    "            pre_stage_channels, num_channels)\r\n",
    "        self.stage4, pre_stage_channels = self._make_stage(\r\n",
    "            self.stage4_cfg, num_channels, multi_scale_output=False)\r\n",
    "\r\n",
    "        self.final_layer = nn.Conv2d(\r\n",
    "            in_channels=pre_stage_channels[0],\r\n",
    "            out_channels=cfg['MODEL']['NUM_JOINTS'],\r\n",
    "            kernel_size=extra['FINAL_CONV_KERNEL'],\r\n",
    "            stride=1,\r\n",
    "            padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0\r\n",
    "        )\r\n",
    "\r\n",
    "        self.pretrained_layers = extra['PRETRAINED_LAYERS']\r\n",
    "\r\n",
    "    def _make_transition_layer(\r\n",
    "            self, num_channels_pre_layer, num_channels_cur_layer):\r\n",
    "        num_branches_cur = len(num_channels_cur_layer)\r\n",
    "        num_branches_pre = len(num_channels_pre_layer)\r\n",
    "\r\n",
    "        transition_layers = []\r\n",
    "        for i in range(num_branches_cur):\r\n",
    "            if i < num_branches_pre:\r\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\r\n",
    "                    transition_layers.append(\r\n",
    "                        nn.Sequential(\r\n",
    "                            nn.Conv2d(\r\n",
    "                                num_channels_pre_layer[i],\r\n",
    "                                num_channels_cur_layer[i],\r\n",
    "                                3, 1, 1, bias=False\r\n",
    "                            ),\r\n",
    "                            nn.BatchNorm2d(num_channels_cur_layer[i]),\r\n",
    "                            nn.ReLU(inplace=True)\r\n",
    "                        )\r\n",
    "                    )\r\n",
    "                else:\r\n",
    "                    transition_layers.append(None)\r\n",
    "            else:\r\n",
    "                conv3x3s = []\r\n",
    "                for j in range(i+1-num_branches_pre):\r\n",
    "                    inchannels = num_channels_pre_layer[-1]\r\n",
    "                    outchannels = num_channels_cur_layer[i] \\\r\n",
    "                        if j == i-num_branches_pre else inchannels\r\n",
    "                    conv3x3s.append(\r\n",
    "                        nn.Sequential(\r\n",
    "                            nn.Conv2d(\r\n",
    "                                inchannels, outchannels, 3, 2, 1, bias=False\r\n",
    "                            ),\r\n",
    "                            nn.BatchNorm2d(outchannels),\r\n",
    "                            nn.ReLU(inplace=True)\r\n",
    "                        )\r\n",
    "                    )\r\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\r\n",
    "\r\n",
    "        return nn.ModuleList(transition_layers)\r\n",
    "\r\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\r\n",
    "        downsample = None\r\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\r\n",
    "            downsample = nn.Sequential(\r\n",
    "                nn.Conv2d(\r\n",
    "                    self.inplanes, planes * block.expansion,\r\n",
    "                    kernel_size=1, stride=stride, bias=False\r\n",
    "                ),\r\n",
    "                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\r\n",
    "            )\r\n",
    "\r\n",
    "        layers = []\r\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\r\n",
    "        self.inplanes = planes * block.expansion\r\n",
    "        for i in range(1, blocks):\r\n",
    "            layers.append(block(self.inplanes, planes))\r\n",
    "\r\n",
    "        return nn.Sequential(*layers)\r\n",
    "\r\n",
    "    def _make_stage(self, layer_config, num_inchannels,\r\n",
    "                    multi_scale_output=True):\r\n",
    "        num_modules = layer_config['NUM_MODULES']\r\n",
    "        num_branches = layer_config['NUM_BRANCHES']\r\n",
    "        num_blocks = layer_config['NUM_BLOCKS']\r\n",
    "        num_channels = layer_config['NUM_CHANNELS']\r\n",
    "        block = blocks_dict[layer_config['BLOCK']]\r\n",
    "        fuse_method = layer_config['FUSE_METHOD']\r\n",
    "\r\n",
    "        modules = []\r\n",
    "        for i in range(num_modules):\r\n",
    "            # multi_scale_output is only used last module\r\n",
    "            if not multi_scale_output and i == num_modules - 1:\r\n",
    "                reset_multi_scale_output = False\r\n",
    "            else:\r\n",
    "                reset_multi_scale_output = True\r\n",
    "\r\n",
    "            modules.append(\r\n",
    "                HighResolutionModule(\r\n",
    "                    num_branches,\r\n",
    "                    block,\r\n",
    "                    num_blocks,\r\n",
    "                    num_inchannels,\r\n",
    "                    num_channels,\r\n",
    "                    fuse_method,\r\n",
    "                    reset_multi_scale_output\r\n",
    "                )\r\n",
    "            )\r\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\r\n",
    "\r\n",
    "        return nn.Sequential(*modules), num_inchannels\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = self.bn1(x)\r\n",
    "        x = self.relu(x)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = self.bn2(x)\r\n",
    "        x = self.relu(x)\r\n",
    "        x = self.layer1(x)\r\n",
    "\r\n",
    "        x_list = []\r\n",
    "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\r\n",
    "            if self.transition1[i] is not None:\r\n",
    "                x_list.append(self.transition1[i](x))\r\n",
    "            else:\r\n",
    "                x_list.append(x)\r\n",
    "        y_list = self.stage2(x_list)\r\n",
    "\r\n",
    "        x_list = []\r\n",
    "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\r\n",
    "            if self.transition2[i] is not None:\r\n",
    "                x_list.append(self.transition2[i](y_list[-1]))\r\n",
    "            else:\r\n",
    "                x_list.append(y_list[i])\r\n",
    "        y_list = self.stage3(x_list)\r\n",
    "\r\n",
    "        x_list = []\r\n",
    "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\r\n",
    "            if self.transition3[i] is not None:\r\n",
    "                x_list.append(self.transition3[i](y_list[-1]))\r\n",
    "            else:\r\n",
    "                x_list.append(y_list[i])\r\n",
    "        y_list = self.stage4(x_list)\r\n",
    "\r\n",
    "        x = self.final_layer(y_list[0])\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "    def init_weights(self, pretrained=''):\r\n",
    "        print('=> init weights from normal distribution')\r\n",
    "        for m in self.modules():\r\n",
    "            if isinstance(m, nn.Conv2d):\r\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
    "                nn.init.normal_(m.weight, std=0.001)\r\n",
    "                for name, _ in m.named_parameters():\r\n",
    "                    if name in ['bias']:\r\n",
    "                        nn.init.constant_(m.bias, 0)\r\n",
    "            elif isinstance(m, nn.BatchNorm2d):\r\n",
    "                nn.init.constant_(m.weight, 1)\r\n",
    "                nn.init.constant_(m.bias, 0)\r\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\r\n",
    "                nn.init.normal_(m.weight, std=0.001)\r\n",
    "                for name, _ in m.named_parameters():\r\n",
    "                    if name in ['bias']:\r\n",
    "                        nn.init.constant_(m.bias, 0)\r\n",
    "\r\n",
    "        if os.path.isfile(pretrained):\r\n",
    "            pretrained_state_dict = torch.load(pretrained)\r\n",
    "            print('=> loading pretrained model {}'.format(pretrained))\r\n",
    "\r\n",
    "            need_init_state_dict = {}\r\n",
    "            for name, m in pretrained_state_dict.items():\r\n",
    "                if name.split('.')[0] in self.pretrained_layers \\\r\n",
    "                   or self.pretrained_layers[0] is '*':\r\n",
    "                    need_init_state_dict[name] = m\r\n",
    "            self.load_state_dict(need_init_state_dict, strict=False)\r\n",
    "        elif pretrained:\r\n",
    "            print('=> please download pre-trained models first!')\r\n",
    "            raise ValueError('{} is not exist!'.format(pretrained))\r\n",
    "\r\n",
    "\r\n",
    "def get_pose_net(cfg, is_train, **kwargs):\r\n",
    "    model = PoseHighResolutionNet(cfg, **kwargs)\r\n",
    "\r\n",
    "    if is_train and cfg['MODEL']['INIT_WEIGHTS']:\r\n",
    "        model.init_weights(cfg['MODEL']['PRETRAINED'])\r\n",
    "    else:\r\n",
    "        model.load_state_dict(torch.load(cfg['MODEL']['PRETRAINED']))\r\n",
    "\r\n",
    "    return model\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\r\n",
    "\r\n",
    "\r\n",
    "def model_define(yaml_path, train=True):\r\n",
    "  with open(yaml_path) as f:\r\n",
    "    cfg = yaml.load(f)\r\n",
    "\r\n",
    "  model = get_pose_net(cfg, train)\r\n",
    "  return model\r\n",
    "\r\n",
    "\r\n",
    "train_tfms = A.Compose([\r\n",
    "    A.GaussNoise(p=0.5),\r\n",
    "\r\n",
    "    A.OneOf([\r\n",
    "            # A.Blur(p=1.0),\r\n",
    "            A.GaussianBlur(p=1.0),\r\n",
    "            A.MotionBlur(p=1),\r\n",
    "            # A.ImageCompression(p=1.0),\r\n",
    "            ], p=0.5),\r\n",
    "\r\n",
    "    A.OneOf([\r\n",
    "            A.ChannelShuffle(p=1.0),\r\n",
    "            A.HueSaturationValue(p=1.0),\r\n",
    "            # A.RGBShift(p=1.0),\r\n",
    "            ], p=0.5),\r\n",
    "\r\n",
    "    A.OneOf([\r\n",
    "            A.RandomBrightnessContrast(p=1.0),\r\n",
    "            A.RandomContrast(p=1.0),\r\n",
    "            A.RandomGamma(p=1.0),\r\n",
    "            A.CLAHE(p=1.0)\r\n",
    "            ], p=0.5),\r\n",
    "\r\n",
    "    A.Compose([\r\n",
    "        A.Normalize(),\r\n",
    "        # ToTensor()\r\n",
    "    ])\r\n",
    "])\r\n",
    "\r\n",
    "\r\n",
    "valid_tfms = A.Normalize()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/ba50a82dce412df97f088c572d86d7977753bf74/lib/core/inference.py#L18:5\r\n",
    "from numpy.linalg import LinAlgError\r\n",
    "\r\n",
    "\r\n",
    "def get_max_preds(batch_heatmaps):\r\n",
    "    '''\r\n",
    "    get predictions from score maps\r\n",
    "    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\r\n",
    "    '''\r\n",
    "    assert isinstance(batch_heatmaps, np.ndarray), \\\r\n",
    "        'batch_heatmaps should be numpy.ndarray'\r\n",
    "    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\r\n",
    "\r\n",
    "    batch_size = batch_heatmaps.shape[0]\r\n",
    "    num_joints = batch_heatmaps.shape[1]\r\n",
    "    width = batch_heatmaps.shape[3]\r\n",
    "    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\r\n",
    "    idx = np.argmax(heatmaps_reshaped, 2)\r\n",
    "    maxvals = np.amax(heatmaps_reshaped, 2)\r\n",
    "\r\n",
    "    maxvals = maxvals.reshape((batch_size, num_joints, 1))\r\n",
    "    idx = idx.reshape((batch_size, num_joints, 1))\r\n",
    "\r\n",
    "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\r\n",
    "\r\n",
    "    preds[:, :, 0] = (preds[:, :, 0]) % width\r\n",
    "    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\r\n",
    "\r\n",
    "    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\r\n",
    "    pred_mask = pred_mask.astype(np.float32)\r\n",
    "\r\n",
    "    preds *= pred_mask\r\n",
    "    return preds, maxvals\r\n",
    "\r\n",
    "\r\n",
    "def dark_post_processing(coords, batch_heatmaps):\r\n",
    "    '''\r\n",
    "    DARK post-pocessing\r\n",
    "    :param coords: batchsize*num_kps*2\r\n",
    "    :param batch_heatmaps:batchsize*num_kps*high*width\r\n",
    "    :return:\r\n",
    "    '''\r\n",
    "\r\n",
    "    shape_pad = list(batch_heatmaps.shape)\r\n",
    "    shape_pad[2] = shape_pad[2] + 2\r\n",
    "    shape_pad[3] = shape_pad[3] + 2\r\n",
    "\r\n",
    "    for i in range(shape_pad[0]):\r\n",
    "        for j in range(shape_pad[1]):\r\n",
    "            mapij = batch_heatmaps[i, j, :, :]\r\n",
    "            maxori = np.max(mapij)\r\n",
    "            mapij = cv2.GaussianBlur(mapij, (7, 7), 0)\r\n",
    "            max = np.max(mapij)\r\n",
    "            min = np.min(mapij)\r\n",
    "            mapij = (mapij-min)/(max-min) * maxori\r\n",
    "            batch_heatmaps[i, j, :, :] = mapij\r\n",
    "    batch_heatmaps = np.clip(batch_heatmaps, 0.001, 50)\r\n",
    "    batch_heatmaps = np.log(batch_heatmaps)\r\n",
    "    batch_heatmaps_pad = np.zeros(shape_pad, dtype=float)\r\n",
    "    batch_heatmaps_pad[:, :, 1:-1, 1:-1] = batch_heatmaps\r\n",
    "    batch_heatmaps_pad[:, :, 1:-1, -1] = batch_heatmaps[:, :, :, -1]\r\n",
    "    batch_heatmaps_pad[:, :, -1, 1:-1] = batch_heatmaps[:, :, -1, :]\r\n",
    "    batch_heatmaps_pad[:, :, 1:-1, 0] = batch_heatmaps[:, :, :, 0]\r\n",
    "    batch_heatmaps_pad[:, :, 0, 1:-1] = batch_heatmaps[:, :, 0, :]\r\n",
    "    batch_heatmaps_pad[:, :, -1, -1] = batch_heatmaps[:, :, -1, -1]\r\n",
    "    batch_heatmaps_pad[:, :, 0, 0] = batch_heatmaps[:, :, 0, 0]\r\n",
    "    batch_heatmaps_pad[:, :, 0, -1] = batch_heatmaps[:, :, 0, -1]\r\n",
    "    batch_heatmaps_pad[:, :, -1, 0] = batch_heatmaps[:, :, -1, 0]\r\n",
    "    I = np.zeros((shape_pad[0], shape_pad[1]))\r\n",
    "    Ix1 = np.zeros((shape_pad[0], shape_pad[1]))\r\n",
    "    Iy1 = np.zeros((shape_pad[0], shape_pad[1]))\r\n",
    "    Ix1y1 = np.zeros((shape_pad[0], shape_pad[1]))\r\n",
    "    Ix1_y1_ = np.zeros((shape_pad[0], shape_pad[1]))\r\n",
    "    Ix1_ = np.zeros((shape_pad[0], shape_pad[1]))\r\n",
    "    Iy1_ = np.zeros((shape_pad[0], shape_pad[1]))\r\n",
    "    coords = coords.astype(np.int32)\r\n",
    "    for i in range(shape_pad[0]):\r\n",
    "        for j in range(shape_pad[1]):\r\n",
    "            I[i, j] = batch_heatmaps_pad[i, j,\r\n",
    "                                         coords[i, j, 1]+1, coords[i, j, 0]+1]\r\n",
    "            Ix1[i, j] = batch_heatmaps_pad[i, j,\r\n",
    "                                           coords[i, j, 1]+1, coords[i, j, 0] + 2]\r\n",
    "            Ix1_[i, j] = batch_heatmaps_pad[i, j,\r\n",
    "                                            coords[i, j, 1]+1, coords[i, j, 0]]\r\n",
    "            Iy1[i, j] = batch_heatmaps_pad[i, j,\r\n",
    "                                           coords[i, j, 1] + 2, coords[i, j, 0]+1]\r\n",
    "            Iy1_[i, j] = batch_heatmaps_pad[i, j,\r\n",
    "                                            coords[i, j, 1], coords[i, j, 0]+1]\r\n",
    "            Ix1y1[i, j] = batch_heatmaps_pad[i, j,\r\n",
    "                                             coords[i, j, 1] + 2, coords[i, j, 0] + 2]\r\n",
    "            Ix1_y1_[i, j] = batch_heatmaps_pad[i,\r\n",
    "                                               j, coords[i, j, 1], coords[i, j, 0]]\r\n",
    "    dx = 0.5 * (Ix1 - Ix1_)\r\n",
    "    dy = 0.5 * (Iy1 - Iy1_)\r\n",
    "    D = np.zeros((shape_pad[0], shape_pad[1], 2))\r\n",
    "    D[:, :, 0] = dx\r\n",
    "    D[:, :, 1] = dy\r\n",
    "    D.reshape((shape_pad[0], shape_pad[1], 2, 1))\r\n",
    "    dxx = Ix1 - 2*I + Ix1_\r\n",
    "    dyy = Iy1 - 2*I + Iy1_\r\n",
    "    dxy = 0.5*(Ix1y1 - Ix1 - Iy1 + I + I - Ix1_-Iy1_+Ix1_y1_)\r\n",
    "    hessian = np.zeros((shape_pad[0], shape_pad[1], 2, 2))\r\n",
    "    hessian[:, :, 0, 0] = dxx\r\n",
    "    hessian[:, :, 1, 0] = dxy\r\n",
    "    hessian[:, :, 0, 1] = dxy\r\n",
    "    hessian[:, :, 1, 1] = dyy\r\n",
    "    inv_hessian = np.zeros(hessian.shape)\r\n",
    "    # hessian_test = np.zeros(hessian.shape)\r\n",
    "    for i in range(shape_pad[0]):\r\n",
    "        for j in range(shape_pad[1]):\r\n",
    "            hessian_tmp = hessian[i, j, :, :]\r\n",
    "            try:\r\n",
    "                inv_hessian[i, j, :, :] = np.linalg.inv(hessian_tmp)\r\n",
    "            except LinAlgError:\r\n",
    "                inv_hessian[i, j, :, :] = np.zeros((2, 2))\r\n",
    "            # hessian_test[i,j,:,:] = np.matmul(hessian[i,j,:,:],inv_hessian[i,j,:,:])\r\n",
    "            # print( hessian_test[i,j,:,:])\r\n",
    "    res = np.zeros(coords.shape)\r\n",
    "    coords = coords.astype(np.float)\r\n",
    "    for i in range(shape_pad[0]):\r\n",
    "        for j in range(shape_pad[1]):\r\n",
    "            D_tmp = D[i, j, :]\r\n",
    "            D_tmp = D_tmp[:, np.newaxis]\r\n",
    "            shift = np.matmul(inv_hessian[i, j, :, :], D_tmp)\r\n",
    "            # print(shift.shape)\r\n",
    "            res_tmp = coords[i, j, :] - shift.reshape((-1))\r\n",
    "            res[i, j, :] = res_tmp\r\n",
    "    return res\r\n",
    "\r\n",
    "\r\n",
    "def get_final_preds(cfg, batch_heatmaps):\r\n",
    "    heatmap_height = batch_heatmaps.shape[2]\r\n",
    "    heatmap_width = batch_heatmaps.shape[3]\r\n",
    "    if cfg.target_type == 'gaussian':\r\n",
    "        coords, maxvals = get_max_preds(batch_heatmaps)\r\n",
    "        if cfg.post_processing == \"dark\":\r\n",
    "            coords = dark_post_processing(coords, batch_heatmaps)\r\n",
    "\r\n",
    "    preds = coords.copy()\r\n",
    "    preds[:, :, 0] = preds[:, :, 0] / \\\r\n",
    "        (heatmap_width - 1.0) * (4 * heatmap_width - 1.0)\r\n",
    "    preds[:, :, 1] = preds[:, :, 1] / \\\r\n",
    "        (heatmap_height - 1.0) * (4 * heatmap_height - 1.0)\r\n",
    "\r\n",
    "    return preds\r\n",
    "\r\n",
    "# https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/ba50a82dce412df97f088c572d86d7977753bf74/lib/core/evaluate.py#L41\r\n",
    "\r\n",
    "\r\n",
    "def calc_dists(preds, target, normalize):\r\n",
    "    preds = preds.astype(np.float32)\r\n",
    "    target = target.astype(np.float32)\r\n",
    "    dists = np.zeros((preds.shape[1], preds.shape[0]))\r\n",
    "    for n in range(preds.shape[0]):\r\n",
    "        for c in range(preds.shape[1]):\r\n",
    "            if target[n, c, 0] > 1 and target[n, c, 1] > 1:\r\n",
    "                normed_preds = preds[n, c, :] / normalize[n]\r\n",
    "                normed_targets = target[n, c, :] / normalize[n]\r\n",
    "                dists[c, n] = np.linalg.norm(normed_preds - normed_targets)\r\n",
    "            else:\r\n",
    "                dists[c, n] = -1\r\n",
    "    return dists\r\n",
    "\r\n",
    "\r\n",
    "def dist_acc(dists, thr=0.5):\r\n",
    "    ''' Return percentage below threshold while ignoring values with a -1 '''\r\n",
    "    dist_cal = np.not_equal(dists, -1)\r\n",
    "    num_dist_cal = dist_cal.sum()\r\n",
    "    if num_dist_cal > 0:\r\n",
    "        return np.less(dists[dist_cal], thr).sum() * 1.0 / num_dist_cal\r\n",
    "    else:\r\n",
    "        return -1\r\n",
    "\r\n",
    "\r\n",
    "def accuracy(output, target, hm_type='gaussian', thr=0.5):\r\n",
    "    '''\r\n",
    "    Calculate accuracy according to PCK,\r\n",
    "    but uses ground truth heatmap rather than x,y locations\r\n",
    "    First value to be returned is average accuracy across 'idxs',\r\n",
    "    followed by individual accuracies\r\n",
    "    '''\r\n",
    "    idx = list(range(output.shape[1]))\r\n",
    "    norm = 1.0\r\n",
    "    if hm_type == 'gaussian':\r\n",
    "        pred, _ = get_max_preds(output)\r\n",
    "        target, _ = get_max_preds(target)\r\n",
    "        h = output.shape[2]\r\n",
    "        w = output.shape[3]\r\n",
    "        norm = np.ones((pred.shape[0], 2)) * np.array([h, w]) / 10\r\n",
    "    dists = calc_dists(pred, target, norm)\r\n",
    "\r\n",
    "    acc = np.zeros((len(idx) + 1))\r\n",
    "    avg_acc = 0\r\n",
    "    cnt = 0\r\n",
    "\r\n",
    "    for i in range(len(idx)):\r\n",
    "        acc[i + 1] = dist_acc(dists[idx[i]])\r\n",
    "        if acc[i + 1] >= 0:\r\n",
    "            avg_acc = avg_acc + acc[i + 1]\r\n",
    "            cnt += 1\r\n",
    "\r\n",
    "    avg_acc = avg_acc / cnt if cnt != 0 else 0\r\n",
    "    if cnt != 0:\r\n",
    "        acc[0] = avg_acc\r\n",
    "    return acc, avg_acc, cnt, pred\r\n",
    "\r\n",
    "\r\n",
    "def extract_coordinate(heatmap_outs, image_size=cfg.image_size, num_joints=cfg.num_joints):\r\n",
    "    \"\"\"\r\n",
    "        Extracting coordinate from heatmap\r\n",
    "        \r\n",
    "        [Args]\r\n",
    "            [heatmap_outs]: heatmap result\r\n",
    "\r\n",
    "            [num_joints]: the number of joints\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    heatmap_outs = heatmap_outs.detach().cpu()\r\n",
    "\r\n",
    "    batch_size, _, height, width = heatmap_outs.shape\r\n",
    "    output_shape = (height, width)\r\n",
    "\r\n",
    "    # coordinate extract from output heatmap\r\n",
    "    y = [i for i in range(output_shape[0])]\r\n",
    "    x = [i for i in range(output_shape[1])]\r\n",
    "    xx, yy = np.meshgrid(x, y)\r\n",
    "\r\n",
    "    xx = torch.from_numpy(xx).float() + 1\r\n",
    "    yy = torch.from_numpy(yy).float() + 1\r\n",
    "\r\n",
    "    heatmap_outs = torch.reshape(heatmap_outs, [batch_size, num_joints, -1])\r\n",
    "    heatmap_outs = F.softmax(heatmap_outs, dim=-1)\r\n",
    "    heatmap_outs = torch.reshape(\r\n",
    "        heatmap_outs, [batch_size, num_joints, output_shape[0], output_shape[1]])\r\n",
    "\r\n",
    "    x_out = torch.sum(\r\n",
    "        torch.mul(heatmap_outs,\r\n",
    "                  torch.reshape(xx, [1, 1, output_shape[0], output_shape[1]]).repeat([batch_size, num_joints, 1, 1])), [2, 3]\r\n",
    "    )\r\n",
    "    y_out = torch.sum(\r\n",
    "        torch.mul(heatmap_outs,\r\n",
    "                  torch.reshape(yy, [1, 1, output_shape[0], output_shape[1]]).repeat([batch_size, num_joints, 1, 1])), [2, 3]\r\n",
    "    )\r\n",
    "\r\n",
    "    coord_out = torch.cat(\r\n",
    "        [torch.reshape(x_out, [batch_size, num_joints, 1]),\r\n",
    "         torch.reshape(y_out, [batch_size, num_joints, 1])],\r\n",
    "        axis=2\r\n",
    "    )\r\n",
    "    coord_out = coord_out - 1\r\n",
    "    coord_out = coord_out / output_shape[0] * image_size[0]\r\n",
    "    return coord_out.float()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, yaml_name='w48_384x288_adam_lr1e-3.yaml'):\r\n",
    "  # for reporduction\r\n",
    "  seed = cfg.seed\r\n",
    "  torch.cuda.empty_cache()\r\n",
    "\r\n",
    "  torch.manual_seed(seed)\r\n",
    "  torch.cuda.manual_seed(seed)\r\n",
    "  torch.cuda.manual_seed_all(seed)  # if use multi-GPU\r\n",
    "  torch.backends.cudnn.deterministic = True\r\n",
    "  torch.backends.cudnn.benchmark = False\r\n",
    "  np.random.seed(seed)\r\n",
    "  random.seed(seed)\r\n",
    "  np.random.seed(seed)\r\n",
    "\r\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "\r\n",
    "  yaml_path = os.path.join('./experiments/coco/hrnet', yaml_name)\r\n",
    "  model = model_define(yaml_path, cfg.init_training)\r\n",
    "  model = model.to(device)\r\n",
    "\r\n",
    "  if cfg.loss_type == \"OHKMMSE\":\r\n",
    "    heatmap_criterion = JointsOHKMMSELoss()\r\n",
    "  elif cfg.loss_type == \"MSE\":\r\n",
    "    heatmap_criterion = JointsMSELoss()\r\n",
    "  rmse_criterion = JointsRMSELoss()\r\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\r\n",
    "  # optimizer = optim.RAdam(model.parameters(), lr=cfg.lr)\r\n",
    "  scheduler = torch.optim.lr_scheduler.MultiStepLR(\r\n",
    "      optimizer, milestones=[90, 120], gamma=0.1, last_epoch=-1)\r\n",
    "\r\n",
    "  total_df = pd.read_csv(meta_info_dir)\r\n",
    "\r\n",
    "  def making_sector_label(image_name):\r\n",
    "      sector_name = image_name.split('-')[0]\r\n",
    "      return sector_name\r\n",
    "\r\n",
    "  total_df['sector'] = total_df.apply(\r\n",
    "      lambda x: making_sector_label(x['image']), axis=1\r\n",
    "  )\r\n",
    "\r\n",
    "  columns = total_df.columns.tolist()\r\n",
    "  columns = columns[-1:] + columns[:-1]\r\n",
    "  total_df = total_df[columns]\r\n",
    "\r\n",
    "  if cfg.startify:\r\n",
    "    train_df, valid_df = train_test_split(\r\n",
    "        total_df.iloc[:, 1:], test_size=cfg.test_ratio, random_state=seed, stratify=total_df.iloc[:, 0])\r\n",
    "  else:\r\n",
    "    train_df, valid_df = train_test_split(\r\n",
    "        total_df.iloc[:, 1:], test_size=cfg.test_ratio, random_state=seed)\r\n",
    "\r\n",
    "  train_ds = DaconKeypointsDataset(train_img_path, train_df, train_tfms)\r\n",
    "  valid_ds = DaconKeypointsDataset(train_img_path, valid_df, valid_tfms)\r\n",
    "  train_dl = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\r\n",
    "  valid_dl = DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=False)\r\n",
    "\r\n",
    "  best_loss = float('INF')\r\n",
    "  for epoch in range(cfg.epochs):\r\n",
    "      ################\r\n",
    "      #    Train     #\r\n",
    "      ################\r\n",
    "      with tqdm(train_dl, total=train_dl.__len__(), unit=\"batch\") as train_bar:\r\n",
    "          train_acc_list = []\r\n",
    "          train_loss_list = []\r\n",
    "          train_rmse_list = []\r\n",
    "          train_heatmap_list = []\r\n",
    "          for sample in train_bar:\r\n",
    "              train_bar.set_description(f\"Train Epoch {epoch+1}\")\r\n",
    "\r\n",
    "              optimizer.zero_grad()\r\n",
    "              images, targ_coords = sample['image'].to(\r\n",
    "                  device), sample['keypoints'].to(device)\r\n",
    "              target, target_weight = sample['target'].to(\r\n",
    "                  device), sample['target_weight'].to(device)\r\n",
    "\r\n",
    "              model.train()\r\n",
    "              with torch.set_grad_enabled(True):\r\n",
    "                  preds = model(images)\r\n",
    "                  heatmap_loss = heatmap_criterion(\r\n",
    "                      preds, target, target_weight)\r\n",
    "\r\n",
    "                  if cfg.extract == \"pose_fix\":\r\n",
    "                    pred_coords = extract_coordinate(preds)\r\n",
    "                  else:\r\n",
    "                    heatmap_height = preds.shape[2]\r\n",
    "                    heatmap_width = preds.shape[3]\r\n",
    "                    pred_coords, _ = get_max_preds(\r\n",
    "                        preds.detach().cpu().numpy())\r\n",
    "                    pred_coords[:, :, 0] = pred_coords[:, :, 0] / \\\r\n",
    "                        (heatmap_width - 1.0) * (4 * heatmap_width - 1.0)\r\n",
    "                    pred_coords[:, :, 1] = pred_coords[:, :, 1] / \\\r\n",
    "                        (heatmap_height - 1.0) * (4 * heatmap_height - 1.0)\r\n",
    "\r\n",
    "                  pred_coords = torch.tensor(pred_coords).float().to(device)\r\n",
    "                  rmse_loss = rmse_criterion(pred_coords, targ_coords)\r\n",
    "                  _, avg_acc, cnt, pred = accuracy(preds.detach().cpu().numpy(),\r\n",
    "                                                   target.detach().cpu().numpy())\r\n",
    "\r\n",
    "                  total_loss = heatmap_loss * .5 + rmse_loss * .5\r\n",
    "                  total_loss.backward()\r\n",
    "                  optimizer.step()\r\n",
    "\r\n",
    "                  train_rmse_list.append(float(rmse_loss.item()))\r\n",
    "                  train_heatmap_list.append(float(heatmap_loss.item()))\r\n",
    "                  train_acc_list.append(float(avg_acc))\r\n",
    "                  train_loss_list.append(float(total_loss.item()))\r\n",
    "              train_acc = np.mean(train_acc_list)\r\n",
    "              train_loss = np.mean(train_loss_list)\r\n",
    "              train_rmse = np.mean(train_rmse_list)\r\n",
    "              train_heatmap = np.mean(train_heatmap_list)\r\n",
    "              train_bar.set_postfix(total_loss=train_loss,\r\n",
    "                                    heatmap_loss=train_heatmap,\r\n",
    "                                    rmse_loss=train_rmse,\r\n",
    "                                    train_acc=train_acc)\r\n",
    "            \r\n",
    "\r\n",
    "      ################\r\n",
    "      #    Valid     #\r\n",
    "      ################\r\n",
    "      with tqdm(valid_dl, total=valid_dl.__len__(), unit=\"batch\") as valid_bar:\r\n",
    "          valid_acc_list = []\r\n",
    "          valid_loss_list = []\r\n",
    "          valid_rmse_list = []\r\n",
    "          valid_heatmap_list = []\r\n",
    "          for sample in valid_bar:\r\n",
    "              valid_bar.set_description(f\"Valid Epoch {epoch+1}\")\r\n",
    "\r\n",
    "              images, targ_coords = sample['image'].to(\r\n",
    "                  device), sample['keypoints'].to(device)\r\n",
    "              target, target_weight = sample['target'].to(\r\n",
    "                  device), sample['target_weight'].to(device)\r\n",
    "\r\n",
    "              model.eval()\r\n",
    "              with torch.no_grad():\r\n",
    "                  preds = model(images)\r\n",
    "                  heatmap_loss = heatmap_criterion(\r\n",
    "                      preds, target, target_weight)\r\n",
    "\r\n",
    "                  if cfg.extract == \"pose_fix\":\r\n",
    "                    pred_coords = extract_coordinate(preds)\r\n",
    "                  else:\r\n",
    "                    heatmap_height = preds.shape[2]\r\n",
    "                    heatmap_width = preds.shape[3]\r\n",
    "                    pred_coords = get_final_preds(\r\n",
    "                        cfg, preds.detach().cpu().numpy())\r\n",
    "\r\n",
    "                  pred_coords = torch.tensor(pred_coords).float().to(device)\r\n",
    "                  rmse_loss = rmse_criterion(pred_coords, targ_coords)\r\n",
    "                  _, avg_acc, cnt, pred = accuracy(preds.detach().cpu().numpy(),\r\n",
    "                                                   target.detach().cpu().numpy())\r\n",
    "\r\n",
    "                  valid_rmse_list.append(float(rmse_loss.item()))\r\n",
    "                  valid_acc_list.append(float(avg_acc))\r\n",
    "                  valid_heatmap_list.append(float(heatmap_loss.item()))\r\n",
    "                  valid_loss_list.append(\r\n",
    "                      float(heatmap_loss.item() * 0.5 + rmse_loss.item() * 0.5))\r\n",
    "              valid_acc = np.mean(valid_acc_list)\r\n",
    "              valid_loss = np.mean(valid_loss_list)\r\n",
    "              valid_rmse = np.mean(valid_rmse_list)\r\n",
    "              valid_heatmap = np.mean(valid_heatmap_list)\r\n",
    "              valid_bar.set_postfix(total_loss=valid_loss,\r\n",
    "                                    valid_acc=valid_acc,\r\n",
    "                                    rmse_loss=valid_rmse,\r\n",
    "                                    heatmap_loss=valid_heatmap)\r\n",
    "      scheduler.step()\r\n",
    "\r\n",
    "      if best_loss > valid_rmse:\r\n",
    "          best_model = model\r\n",
    "          save_dir = main_dir\r\n",
    "          save_name = f'best_model.pth'\r\n",
    "          torch.save(model.state_dict(), os.path.join(save_dir, save_name))\r\n",
    "          print(f\"RMSE: {valid_rmse:.4f}\\nBest Model saved.\")\r\n",
    "          best_loss = valid_rmse\r\n",
    "\r\n",
    "  return best_model\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\r\n",
    "torch.cuda.empty_cache()\r\n",
    "cfg = SingleModelConfig(\r\n",
    "    learning_rate=1e-3,\r\n",
    "    init_training=True,\r\n",
    "    startify=False,\r\n",
    "    input_size=[384, 288],\r\n",
    "    extract='pose_fix',\r\n",
    "    sigma=2.0,\r\n",
    "    # loss_type = \"OHKMMSE\",\r\n",
    "    loss_type=\"MSE\",\r\n",
    "    save_folder='single_hrdnet_w48_384x288_PoseFix-Extract_3sigma'\r\n",
    ")\r\n",
    "best_model = train(cfg, yaml_name='w48_384x288_adam_lr1e-3.yaml')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> init weights from normal distribution\n",
      "=> loading pretrained model models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:48<00:00,  1.08s/batch]\n"
     ]
    }
   ],
   "source": [
    "# img = cv2.imread(os.path.join(test_img_path, '649-2-4-32-Z148_A-0000001.jpg'),cv2.COLOR_BGR2RGB)\r\n",
    "test_df=pd.read_csv('./data/sample_submission.csv')\r\n",
    "test_ds = DaconKeypointsDataset(test_img_path, test_df, valid_tfms)\r\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=False)\r\n",
    "ans=list()\r\n",
    "device='cuda' if torch.cuda.is_available() else'cpu'\r\n",
    "yaml_name = 'w48_384x288_adam_lr1e-3.yaml'\r\n",
    "yaml_path = os.path.join('./experiments/coco/hrnet', yaml_name)\r\n",
    "model = model_define(yaml_path, True).to(device)\r\n",
    "d = torch.load(\"./data/best_model.pth\")\r\n",
    "model.load_state_dict(d)\r\n",
    "model.eval()\r\n",
    "with tqdm(test_dl, total=test_dl.__len__(), unit=\"batch\") as test_bar:\r\n",
    "    for sample in test_bar:\r\n",
    "        images, targ_coords = sample['image'].to(device), sample['keypoints'].to(device)\r\n",
    "        model.eval()\r\n",
    "        with torch.no_grad():\r\n",
    "            preds = model(images)\r\n",
    "            pred_coords = extract_coordinate(preds)\r\n",
    "            ans.append(pred_coords.detach().cpu().numpy())\r\n",
    "    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1600, 24, 2)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans=np.array(ans)\r\n",
    "ans=ans.reshape(-1,24,2)\r\n",
    "# ans[:,:,0]*=w/384\r\n",
    "# ans{:,:,1}*=h/288"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}