### 신경망을 수식으로 분해해보자
- 지난 시간까지 데이터를 선형모데롤 해석하는 방법을 배웠다면 이제부턴 비선형모델인 신경망(neural network)을 배워보겠습니다
> 소프트 맥스 연산
    -softmax함수는 모델의 출력을 확률로 해석 할 수 있게 변환해 주는 연산입니다
    - 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측합니다 
    -  신경망은 선형모델과 활성함수(activation function)을 합성한 함수입니다

import numpy as np
def softmax(vec):
    denumerator=np.exp(vec-np.max(vec,axis=-1,keepdims=True))
    numerator =np.sum(denumerator,axis=-1,keepdims=True)
    val=denumerator/numerator
    return val

vec=np.array([[1,2,0],[-1,0,1],[-10,0,10]])
print(softmax(vec))
### 활성함수가 뭐에요>
- 활성함수는 R위에 정의된 비선형 함수로서 딥러닝에서 매우 중요한 개념이다
- 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없습니다
- 시그모이드 함수나 tanh 함수는 전통적으로 많이 쓰이던 활섬하수지만 딥러닝에선 ReLU함수를 많이 쓰고있다.
- 다츨(multi-layer) 퍼셉트론(Mlp)은 신경망이 여러층 합성된 함수입니다
### 왜 층을 여러개를 쌓나요?
- 이론적으로는 2층 신경망으로도 임이의 연속함수를 근사 할 수 있습니다
- 그러나 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런의 숫자가 훨씬 빨리 줄어들어 좀더 효율적인 학습이 가능합니다
### 딥러닝 학습 원리: 역전파 알고리즘
- 딥러닝은 역전파 알고리즘을 이용하여 각 층에 사용된 패러미터를 학습합니다
- 각 층 패러미터의 그레디언트 벡터는 윗층부터 역순으로 계산하게 됩니다.
- 역전파 알고리즘은 합성함수 미분법인 연쇄법칙 기반 자동미분을 사용합니다 
 
  