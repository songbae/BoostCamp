> 모수가 뭐에요?
    - 통계적 모델링은 적절한 가정 위에서 확률분포를 추정하는 것이 목표이며 , 기계학습과 통계학이
    공통적으로 추구하는 목표입니다.
    -  그러나 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아낸다는 것은 불가능하므로, 근사적으로 확률분포를 추정할 수 밖에 없습니다
    -  데이터가 특정 확률분포를 따른다고 선험적으로 가정하 후 그 분포를 결정하는 모수를 추정하는 방법을 모수적 방법론이라고 합니다
    -  특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 비모수 방법론이라고 부릅니다
    ### 확률분포 가정 하기 : 예제
        > 확률분포를 가정하는 방법: 운선 히스토그램을 통해 모양을 관찰한다
            - 데이터가 2개의 값(0,1)만 가지는 경우 ->베르누이 분포
            - 데이터가 n개의 이산적인 값을 가지는 경우-> 카테고리 분포
            - 데이터가 [0,1]사이에서 값을 가지는 경우-> 베타분포
            - 대이터가 0 이상의 값을 가지는 경우-> 감마분포,로그정규분포 등
            - 데이터가 R전체에서 값을 가지는 경우-> 정규분포, 라플라스분포 등
        - 기계적으로 확률분포를 가정해서는 안되며 , 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙이다.
        > 데이터로 모수를 추정해보자
            - 데이터의 확률분포를 가정했다면 모수를 추정해볼 수 있습니다
            - 정규분포릐 모수는 평균 u와 분산 o^2으로 이를 추정하는 통계량(statistic)은 다음과 같다. 
    ### 최대가능도 추정법
        - 표본평균이나 표본분산을 중요한 통계량이지만 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게 됩니다.
        - 이론적으로 가장 가능성이 높은 못를 추정하는 방법중 하나는 **최대가능도 추정법(MLE)** 입니다
        - 데이터 집합 X가 독립적으로 추출되었을 경우 로그가능도를 최적화합니다.
    ### 왜 로그가능도를 사용하나요?
        - 로그가능도를 최적화하는 모수 O는 가능도를 최적화하는 MLE가 됩니다.
        - 데이터의 숫자가 적으면 상관없지만 만일 데이터의 숫자가 수억단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것은 불가능합니다
        - 데이터가 독립일 경우 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀수 있기 떄문에 컴퓨터로 연산이 가능해집니다.
        - 경사하강법으로 가능도를 최적화할때 미분 연산을 사용하게 되는데, 로그가능도를 사용하면, 연산량을 O(n^2)에서 O(n)으로 줄여줍니다
        - 대게의 손실함수의 경우 경사하강법을 사용하므로 음의 고르가능도를 최적화하게 됩니다. 
    ### 딥러닝에서 최대가능도 추정법
        - 최대가능도 추정법을 이용해서 기계학습 모델을 학습할 수 있다.
        - 디럽닝 모델의 가중치를 라 표기했을때 분류 문제에서 소프트맥스 멕터는 카테고리분포의 모수를 모델링합니다
        - 원핫벡터로 표현한 정답레이블 y=(y1...yk)을 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있습니다.
    ### 확률분포의 거리를 구해보자
        - 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도합니다
        -  데이터공간에 두 개의 확률분포 P(x),Q(x)가 있을 경우 두 확률분포 사이의 거리를 계산할떄 다음과 같은 함수들을 이용합니다.
            - 충변동 거리(Tatal variation Distance , TV)
            - 쿨랙-라이블러 발산(kullback-Leibler Divergence,KL)
            - 바슈타인 거리(Wasserstien Distance)
            