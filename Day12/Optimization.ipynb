{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduction\n",
    "---\n",
    "    - gradient Descent\n",
    "    - First_order iterative optimziation algorithm for finding a local minimum of a differnetiale function\n",
    "---\n",
    "    - generalization \n",
    "    - under-fitting vs over-fitting\n",
    "    - cross validation\n",
    "    - Bias-variance tradeoff\n",
    "    - Bootstrapping\n",
    "    - Bagging and boosting\n",
    "---\n",
    "    Cross-validation\n",
    "    - k-fold validation\n",
    "    - 학습데이터를 나누어서 test_date와 분리한다. \n",
    "---\n",
    "    Bias and variance\n",
    "    - variance: 일관성을 뜻하며 , low 일수록 일관성이 높다는 것을 뜻한다.\n",
    "    - bias: Cost= bias^2 +variance+noise -> 이런식으로 3가지로 이루어져있기 때문에  cost를 줄이기 위해 고려해야 할 것은 3가지가 있다.\n",
    "---\n",
    "    Booststrapping\n",
    "    - any test or metric that uses random sampling whith replacement\n",
    "    - 기본적으로 학습데이터가 고정되어있을 떄 subsampling을 통해 여러 개로 분류해서 trainging 한다\n",
    "---\n",
    "    Bagging vs boosting\n",
    "    -Bagging( Bootstrapping aggregating)\n",
    "        - multiple models are being trained with Booststrapping\n",
    "        - Base classifiers are fitted on random subset where individual predictions are aggregated\n",
    "    - boosting\n",
    "        - 분류하기 어려운 sample에 집중한다.\n",
    "        - 여러개의 모델을 만들어서 합친다. weak learners 들을 sequence하게 합친다.(하나의 강력한 모델을 만든다, bagging 은 여러개의 모델을 병렬적으로 실행)\n",
    "---\n",
    "    gradient descent Methods\n",
    "---\n",
    "    Batch-size Matters\n",
    "    - momentum\n",
    "    - adam\n",
    "---\n",
    "    Regularization\n",
    "    - Early stopping\n",
    "        - 빨리멈춰서 error율을 확인한다\n",
    "    - parameter norm penalty\n",
    "        -숫자들을 작게하자 \n",
    "    - data augmentation\n",
    "        - 더 많은 데이터가 더 좋다. (데이터 증가시키기)\n",
    "        - mnist와 같이 label이 바뀌는 경우엔 rotate ,flip, crop 등이 label을 바꿀수 있기 떄문에 안되지만 목적에 따라서 사용가능 하다.\n",
    "    - noise robustness\n",
    "        - add random noises input or weight ( 일부러 노이즈를 계속 추가하는 방법)\n",
    "    - label smoothing\n",
    "        - 학습데이터 몇개를 뽑아서 섞어준다.\n",
    "    - Dropout\n",
    "        -randomly set units zero\n",
    "    - Batch normalization\n",
    "        - batch norm, layer norm, instance norm, group norm\n",
    "        - 분류문제에 있어서는 일반적으로 성능이 좋다 ( 확실하진 않다.)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.3.0 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from matplotlib==3.3.0) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from matplotlib==3.3.0) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from matplotlib==3.3.0) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from matplotlib==3.3.0) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from matplotlib==3.3.0) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from matplotlib==3.3.0) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib==3.3.0) (1.15.0)\n",
      "Collecting torch\n",
      "  Downloading torch-1.7.1-cp38-cp38-win_amd64.whl (184.0 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib==3.3.0\n",
    "!pip install torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %%\n",
    "from IPython import get_ipython\n",
    "\n",
    "# %%\n",
    "get_ipython().system('pip install matplotlib==3.1.3')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "get_ipython().magic('matplotlib inline')\n",
    "get_ipython().magic(\"config InlineBackend.figure_format='retina'\")\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))\n",
    "n_data = 10000\n",
    "x_numpy = -3+6*np.random.rand(n_data,1)\n",
    "y_numpy = np.exp(-(x_numpy**2))*np.cos(10*x_numpy) + 3e-2*np.random.randn(n_data,1)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_numpy,y_numpy,'r.',ms=2)\n",
    "plt.show()\n",
    "x_torch = torch.Tensor(x_numpy).to(device)\n",
    "y_torch = torch.Tensor(y_numpy).to(device)\n",
    "print (\"Done.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### define model\n",
    "\n",
    "# %%\n",
    "class Model(nn.Module):\n",
    "    def __init__ (self,name='mlp',xdim=1,hdims=[16,16],ydim=1):\n",
    "        super(Model,self).__init__()\n",
    "        self.name=name\n",
    "        self.xdim=xdim\n",
    "        self.hdims=hdims\n",
    "        self.ydim=ydim\n",
    "\n",
    "        self.layers=[]\n",
    "        prev_hdim=self.xdim\n",
    "        for hdim in self.hdims:\n",
    "            self.layers.append(nn.Linear(prev_hdim,hdim))\n",
    "            self.layers.append(nn.Tanh())# activation 함수 즉 활성화 함수지 \n",
    "            prev_hdim=hdim\n",
    "        #final Layer(without activation)\n",
    "        self.layers.append(nn.Linear(prev_hdim,self.ydim,bias=True))\n",
    "        #Concatenate all layers\n",
    "        self.net =nn.Sequential()\n",
    "        for idx,layer in enumerate(self.layers):\n",
    "            layer_name='%s_%02d'%(type(layer).__name__.lower(),idx)\n",
    "            self.net.add_module(layer_name,layer)\n",
    "        \n",
    "        self.init_param()# initialize parameters\n",
    "    \n",
    "    def init_param(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m,nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "    print('done')\n",
    "\n",
    "\n",
    "# %%\n",
    "get_ipython().run_cell_magic('time', '', \"lrate=1e-2\\n#instantiate models\\nmodel_sgd=Model(name='mlp_sgd',xdim=1,hdims=[64,64],ydim=1).to(device)\\nmodel_momentum=Model(name='mlp_momentum',xdim=1,hdims=[64,64],ydim=1).to(device)\\nmodel_adam=Model(name='mlp-adam',xdim=1,hdims=[64,64],ydim=1).to(device)\\n#Optimizers\\nloss=nn.MSELoss()\\noptm_sgd=optim.SGD(\\n    #fill in here\\n    model_sgd.parameters(),lr=lrate\\n\\n)\\noptm_momentum= optim.SGD(\\n    #fill in here\\n    model_momentum.parameters() ,lr=lrate,momentum=0.99\\n\\n)\\noptm_adam=optim.Adam(\\n    #fill in here\\n    model_adam.parameters(),lr=lrate\\n)\\n\\nprint('done')\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## check Parameters\n",
    "# \n",
    "\n",
    "# %%\n",
    "np.set_printoptions(precision=3)\n",
    "n_param=0\n",
    "for idx,(param_name,param) in enumerate(model_sgd.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy=param.detach().cpu().numpy()# to numpy array\n",
    "        n_param+=len(param_numpy.reshape(-1))\n",
    "        print(f'{idx} name:{param_name} shape:{param_numpy.shape}')\n",
    "        print(f'val:{param_numpy.reshape(-1)[:5]}')\n",
    "print(f'Total number of parameters:{format(n_param,\",d\")}')\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "MAX_ITER,BATCH_SIZE,PLOT_EVERY = 1e4,64,500\n",
    "\n",
    "model_sgd.init_param()\n",
    "model_momentum.init_param()\n",
    "model_adam.init_param()\n",
    "\n",
    "model_sgd.train()\n",
    "model_momentum.train()\n",
    "model_adam.train()\n",
    "\n",
    "for it in range(int(MAX_ITER)):\n",
    "    r_idx = np.random.permutation(n_data)[:BATCH_SIZE]\n",
    "    batch_x,batch_y = x_torch[r_idx],y_torch[r_idx]\n",
    "    \n",
    "    # Update with Adam\n",
    "    y_pred_adam = model_adam.forward(batch_x)\n",
    "    loss_adam = loss(y_pred_adam,batch_y)\n",
    "    optm_adam.zero_grad()\n",
    "    loss_adam.backward()\n",
    "    optm_adam.step()\n",
    "\n",
    "    # Update with Momentum\n",
    "    y_pred_momentum = model_momentum.forward(batch_x)\n",
    "    loss_momentum = loss(y_pred_momentum,batch_y)\n",
    "    optm_momentum.zero_grad()\n",
    "    loss_momentum.backward()\n",
    "    optm_momentum.step()\n",
    "\n",
    "    # Update with SGD\n",
    "    y_pred_sgd = model_sgd.forward(batch_x)\n",
    "    loss_sgd = loss(y_pred_sgd,batch_y)\n",
    "    optm_sgd.zero_grad()\n",
    "    loss_sgd.backward()\n",
    "    optm_sgd.step()\n",
    "    \n",
    "\n",
    "    # Plot\n",
    "    if ((it%PLOT_EVERY)==0) or (it==0) or (it==(MAX_ITER-1)):\n",
    "        with torch.no_grad():\n",
    "            y_sgd_numpy = model_sgd.forward(x_torch).cpu().detach().numpy()\n",
    "            y_momentum_numpy = model_momentum.forward(x_torch).cpu().detach().numpy()\n",
    "            y_adam_numpy = model_adam.forward(x_torch).cpu().detach().numpy()\n",
    "            \n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.plot(x_numpy,y_numpy,'r.',ms=4,label='GT')\n",
    "            plt.plot(x_numpy,y_sgd_numpy,'g.',ms=2,label='SGD')\n",
    "            plt.plot(x_numpy,y_momentum_numpy,'b.',ms=2,label='Momentum')\n",
    "            plt.plot(x_numpy,y_adam_numpy,'k.',ms=2,label='ADAM')\n",
    "            plt.title(\"[%d/%d]\"%(it,MAX_ITER),fontsize=15)\n",
    "            plt.legend(loc='upper right',fontsize=15)\n",
    "            plt.show()\n",
    "\n",
    "print (\"Done.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "help(plt.legend)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}