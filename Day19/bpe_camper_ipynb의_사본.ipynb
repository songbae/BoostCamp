{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bpe_camper.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNGuOpGM5VZD"
      },
      "source": [
        "## 2-1.build_bpe 함수를 완성해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koQ-w1sV34sz"
      },
      "source": [
        "# Natural Language Processing\r\n",
        "## Assignment 4: Byte Pair Encoding\r\n",
        "\r\n",
        "### 1. Introduction\r\n",
        "\r\n",
        "- 일반적으로 하나의 단어에 대해 하나의 embedding을 생성할 경우 out-of-vocabulary(OOV)라는 치명적인 문제를 갖게 됩니다. 학습 데이터에서 등장하지 않은 단어가 나오는 경우 Unknown token으로 처리해주어 모델의 입력으로 넣게 되면서 전체적으로 모델의 성능이 저하될 수 있습니다. 반면 모든 단어의 embedding을 만들기에는 필요한 embedding parameter의 수가 지나치게 많습니다.\r\n",
        "이러한 문제를 해결하기 위해 컴퓨터가 이해하는 단어를 표현하는 데에 데이터 압축 알고리즘 중 하나인 byte pair encoding 기법을 적용한 sub-word tokenizaiton이라는 개념이 나타났습니다. \r\n",
        "- 본 과제에서는 byte pair encoding을 이용한 간단한 sub-word tokenizer를 구현해봅니다.\r\n",
        "과제 노트북의 지시사항과 각 함수의 docstring과 [논문](https://arxiv.org/pdf/1508.07909.pdf)의 3페이지 algorithm 1 참고하여 build_bpe 함수를 완성하고 모든 test case를 통과해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9go8KbPe3s-L"
      },
      "source": [
        "from typing import List, Dict, Set\n",
        "from itertools import chain\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "def build_bpe(\n",
        "        corpus: List[str],\n",
        "        max_vocab_size: int\n",
        ") -> List[int]:\n",
        "    \"\"\" BPE Vocabulary Builder\n",
        "    Implement vocabulary builder for byte pair encoding.\n",
        "    Please sort your idx2word by subword length in descending manner.\n",
        "\n",
        "    Hint: Counter in collection library would be helpful\n",
        "\n",
        "    Note: If you convert sentences list to word frequence dictionary,\n",
        "          building speed is enhanced significantly because duplicated words are\n",
        "          preprocessed together\n",
        "\n",
        "    Arguments:\n",
        "    corpus -- List of words to build vocab\n",
        "    max_vocab_size -- The maximum size of vocab\n",
        "\n",
        "    Return:\n",
        "    idx2word -- Subword list\n",
        "    \"\"\"\n",
        "    # Special tokens\n",
        "    PAD = BytePairEncoding.PAD_token  # Index of <PAD> must be 0\n",
        "    UNK = BytePairEncoding.UNK_token  # Index of <UNK> must be 1\n",
        "    CLS = BytePairEncoding.CLS_token  # Index of <CLS> must be 2\n",
        "    SEP = BytePairEncoding.SEP_token  # Index of <SEP> must be 3\n",
        "    MSK = BytePairEncoding.MSK_token  # Index of <MSK> must be 4\n",
        "    SPECIAL = [PAD, UNK, CLS, SEP, MSK]\n",
        "\n",
        "    WORD_END = BytePairEncoding.WORD_END  # Use this token as the\n",
        "    def get_stats(vocab):\n",
        "      pairs = defaultdict(int)\n",
        "      for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "          pairs[symbols[i], symbols[i+1]] += freq\n",
        "      return pairs\n",
        "\n",
        "    def merge_vocab(pair, v_in):\n",
        "      v_out = {}\n",
        "      bigram = re.escape(' '.join(pair))\n",
        "      p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "      for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "      return v_out\n",
        "\n",
        "    def make_unigram(vocab):\n",
        "      unigram = set()\n",
        "\n",
        "      for word, _ in vocab.items():\n",
        "            chars = word.split()\n",
        "            for char in chars[:-1]:\n",
        "              unigram.add(char)\n",
        "      return list(unigram)\n",
        "\n",
        "    vocab_counter = Counter(corpus)\n",
        "    vocab = dict([(str(' '.join(x)) + ' ' + WORD_END, y) for (x, y) in vocab_counter.items()])\n",
        "    final_vocab = make_unigram(vocab)\n",
        "\n",
        "    num_merge = max_vocab_size - len(final_vocab) - len(SPECIAL) - 1\n",
        "    for i in range(num_merge):\n",
        "      pairs = get_stats(vocab)\n",
        "      \n",
        "      try:\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        final_vocab.append(''.join(best))\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "      except: break\n",
        "\n",
        "    idx2word = SPECIAL + sorted(final_vocab, key=lambda x:len(x), reverse=True) + [WORD_END]\n",
        "          \n",
        "    \n",
        "    return idx2word"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBJnrNlY5cjW"
      },
      "source": [
        "## 2-2. build_bpe 함수 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG6-h8Wv5KWB"
      },
      "source": [
        "#############################################\n",
        "# Helper functions below. DO NOT MODIFY!    #\n",
        "#############################################\n",
        "\n",
        "class BytePairEncoding(object):\n",
        "    \"\"\" Byte Pair Encoding class\n",
        "    We aren't gonna use this class for encoding. Because it is too slow......\n",
        "    We will use sentence piece Google have made.\n",
        "    Thus, this class is just for special token index reference.\n",
        "    \"\"\"\n",
        "    PAD_token = '<pad>'\n",
        "    PAD_token_idx = 0\n",
        "    UNK_token = '<unk>'\n",
        "    UNK_token_idx = 1\n",
        "    CLS_token = '<cls>'\n",
        "    CLS_token_idx = 2\n",
        "    SEP_token = '<sep>'\n",
        "    SEP_token_idx = 3\n",
        "    MSK_token = '<msk>'\n",
        "    MSK_token_idx = 4\n",
        "\n",
        "    WORD_END = '_'\n",
        "\n",
        "    def __init__(self, corpus: List[List[str]], max_vocab_size: int) -> None:\n",
        "        self.idx2word = build_bpe(corpus, max_vocab_size)\n",
        "\n",
        "    def encode(self, sentence: List[str]) -> List[int]:\n",
        "        return encode(sentence, self.idx2word)\n",
        "\n",
        "    def decoder(self, tokens: List[int]) -> List[str]:\n",
        "        return decode(tokens, self.idx2word)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Testing functions below.                  #\n",
        "#############################################\n",
        "\n",
        "\n",
        "def test_build_bpe():\n",
        "    print(\"======Building BPE Vocab Test Case======\")\n",
        "    PAD = BytePairEncoding.PAD_token\n",
        "    UNK = BytePairEncoding.UNK_token\n",
        "    CLS = BytePairEncoding.CLS_token\n",
        "    SEP = BytePairEncoding.SEP_token\n",
        "    MSK = BytePairEncoding.MSK_token\n",
        "    WORD_END = BytePairEncoding.WORD_END\n",
        "\n",
        "    # First test\n",
        "    corpus = ['abcde']\n",
        "    vocab = build_bpe(corpus, max_vocab_size=15)\n",
        "    assert vocab[:5] == [PAD, UNK, CLS, SEP, MSK], \\\n",
        "        \"Please insert the special tokens properly\"\n",
        "    print(\"The first test passed!\")\n",
        "\n",
        "    # Second test\n",
        "    assert sorted(vocab[5:], key=len, reverse=True) == vocab[5:], \\\n",
        "        \"Please sort your idx2word by subword length in decsending manner.\"\n",
        "    print(\"The second test passed!\")\n",
        "\n",
        "    # Third test\n",
        "    corpus = ['low'] * 5 + ['lower'] * 2 + ['newest'] * 6 + ['widest'] * 3\n",
        "    vocab = set(build_bpe(corpus, max_vocab_size=24))\n",
        "    assert vocab > {PAD, UNK, CLS, SEP, MSK, 'est_', 'low', 'newest_', \\\n",
        "                    'i', 'e', 'n', 't', 'd', 's', 'o', 'l', 'r', 'w',\n",
        "                    WORD_END} and \\\n",
        "           \"low_\" not in vocab and \"wi\" not in vocab and \"id\" not in vocab, \\\n",
        "        \"Your bpe result does not match expected result\"\n",
        "    print(\"The third test passed!\")\n",
        "\n",
        "    # forth test\n",
        "    corpus = ['aaaaaaaaaaaa', 'abababab']\n",
        "    vocab = set(build_bpe(corpus, max_vocab_size=13))\n",
        "    assert vocab == {PAD, UNK, CLS, SEP, MSK, 'aaaaaaaa', 'aaaa', 'abab', 'aa',\n",
        "                     'ab', 'a', 'b', WORD_END}, \\\n",
        "        \"Your bpe result does not match expected result\"\n",
        "    print(\"The forth test passed!\")\n",
        "\n",
        "    # fifth test\n",
        "    corpus = ['abc', 'bcd']\n",
        "    vocab = build_bpe(corpus, max_vocab_size=10000)\n",
        "    assert len(vocab) == 15, \\\n",
        "        \"Your bpe result does not match expected result\"\n",
        "    print(\"The fifth test passed!\")\n",
        "\n",
        "    print(\"All 5 tests passed!\")\n",
        "test_build_bpe()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======Building BPE Vocab Test Case======\nThe first test passed!\nThe second test passed!\nThe third test passed!\nThe forth test passed!\nThe fifth test passed!\nAll 5 tests passed!\n"
          ]
        }
      ]
    }
  ]
}