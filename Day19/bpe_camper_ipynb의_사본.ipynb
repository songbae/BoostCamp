{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bpe_camper.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNGuOpGM5VZD"
      },
      "source": [
        "## 2-1.build_bpe 함수를 완성해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koQ-w1sV34sz"
      },
      "source": [
        "# Natural Language Processing\r\n",
        "## Assignment 4: Byte Pair Encoding\r\n",
        "\r\n",
        "### 1. Introduction\r\n",
        "\r\n",
        "- 일반적으로 하나의 단어에 대해 하나의 embedding을 생성할 경우 out-of-vocabulary(OOV)라는 치명적인 문제를 갖게 됩니다. 학습 데이터에서 등장하지 않은 단어가 나오는 경우 Unknown token으로 처리해주어 모델의 입력으로 넣게 되면서 전체적으로 모델의 성능이 저하될 수 있습니다. 반면 모든 단어의 embedding을 만들기에는 필요한 embedding parameter의 수가 지나치게 많습니다.\r\n",
        "이러한 문제를 해결하기 위해 컴퓨터가 이해하는 단어를 표현하는 데에 데이터 압축 알고리즘 중 하나인 byte pair encoding 기법을 적용한 sub-word tokenizaiton이라는 개념이 나타났습니다. \r\n",
        "- 본 과제에서는 byte pair encoding을 이용한 간단한 sub-word tokenizer를 구현해봅니다.\r\n",
        "과제 노트북의 지시사항과 각 함수의 docstring과 [논문](https://arxiv.org/pdf/1508.07909.pdf)의 3페이지 algorithm 1 참고하여 build_bpe 함수를 완성하고 모든 test case를 통과해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9go8KbPe3s-L"
      },
      "source": [
        "from typing import List, Dict, Set\r\n",
        "from itertools import chain\r\n",
        "import re\r\n",
        "from collections import defaultdict, Counter\r\n",
        "\r\n",
        "\r\n",
        "def build_bpe(\r\n",
        "        corpus: List[str],\r\n",
        "        max_vocab_size: int\r\n",
        ") -> List[int]:\r\n",
        "    \"\"\" BPE Vocabulary Builder\r\n",
        "    Implement vocabulary builder for byte pair encoding.\r\n",
        "    Please sort your idx2word by subword length in descending manner.\r\n",
        "\r\n",
        "    Hint: Counter in collection library would be helpful\r\n",
        "\r\n",
        "    Note: If you convert sentences list to word frequence dictionary,\r\n",
        "          building speed is enhanced significantly because duplicated words are\r\n",
        "          preprocessed together\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "    corpus -- List of words to build vocab\r\n",
        "    max_vocab_size -- The maximum size of vocab\r\n",
        "\r\n",
        "    Return:\r\n",
        "    idx2word -- Subword list\r\n",
        "    \"\"\"\r\n",
        "    # Special tokens\r\n",
        "    PAD = BytePairEncoding.PAD_token  # Index of <PAD> must be 0\r\n",
        "    UNK = BytePairEncoding.UNK_token  # Index of <UNK> must be 1\r\n",
        "    CLS = BytePairEncoding.CLS_token  # Index of <CLS> must be 2\r\n",
        "    SEP = BytePairEncoding.SEP_token  # Index of <SEP> must be 3\r\n",
        "    MSK = BytePairEncoding.MSK_token  # Index of <MSK> must be 4\r\n",
        "    SPECIAL = [PAD, UNK, CLS, SEP, MSK]\r\n",
        "\r\n",
        "    WORD_END = BytePairEncoding.WORD_END  # Use this token as the end of a word\r\n",
        "    # YOUR CODE HERE\r\n",
        "    id2word = None\r\n",
        "    \r\n",
        "    return idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBJnrNlY5cjW"
      },
      "source": [
        "## 2-2. build_bpe 함수 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG6-h8Wv5KWB"
      },
      "source": [
        "#############################################\r\n",
        "# Helper functions below. DO NOT MODIFY!    #\r\n",
        "#############################################\r\n",
        "\r\n",
        "class BytePairEncoding(object):\r\n",
        "    \"\"\" Byte Pair Encoding class\r\n",
        "    We aren't gonna use this class for encoding. Because it is too slow......\r\n",
        "    We will use sentence piece Google have made.\r\n",
        "    Thus, this class is just for special token index reference.\r\n",
        "    \"\"\"\r\n",
        "    PAD_token = '<pad>'\r\n",
        "    PAD_token_idx = 0\r\n",
        "    UNK_token = '<unk>'\r\n",
        "    UNK_token_idx = 1\r\n",
        "    CLS_token = '<cls>'\r\n",
        "    CLS_token_idx = 2\r\n",
        "    SEP_token = '<sep>'\r\n",
        "    SEP_token_idx = 3\r\n",
        "    MSK_token = '<msk>'\r\n",
        "    MSK_token_idx = 4\r\n",
        "\r\n",
        "    WORD_END = '_'\r\n",
        "\r\n",
        "    def __init__(self, corpus: List[List[str]], max_vocab_size: int) -> None:\r\n",
        "        self.idx2word = build_bpe(corpus, max_vocab_size)\r\n",
        "\r\n",
        "    def encode(self, sentence: List[str]) -> List[int]:\r\n",
        "        return encode(sentence, self.idx2word)\r\n",
        "\r\n",
        "    def decoder(self, tokens: List[int]) -> List[str]:\r\n",
        "        return decode(tokens, self.idx2word)\r\n",
        "\r\n",
        "\r\n",
        "#############################################\r\n",
        "# Testing functions below.                  #\r\n",
        "#############################################\r\n",
        "\r\n",
        "\r\n",
        "def test_build_bpe():\r\n",
        "    print(\"======Building BPE Vocab Test Case======\")\r\n",
        "    PAD = BytePairEncoding.PAD_token\r\n",
        "    UNK = BytePairEncoding.UNK_token\r\n",
        "    CLS = BytePairEncoding.CLS_token\r\n",
        "    SEP = BytePairEncoding.SEP_token\r\n",
        "    MSK = BytePairEncoding.MSK_token\r\n",
        "    WORD_END = BytePairEncoding.WORD_END\r\n",
        "\r\n",
        "    # First test\r\n",
        "    corpus = ['abcde']\r\n",
        "    vocab = build_bpe(corpus, max_vocab_size=15)\r\n",
        "    assert vocab[:5] == [PAD, UNK, CLS, SEP, MSK], \\\r\n",
        "        \"Please insert the special tokens properly\"\r\n",
        "    print(\"The first test passed!\")\r\n",
        "\r\n",
        "    # Second test\r\n",
        "    assert sorted(vocab[5:], key=len, reverse=True) == vocab[5:], \\\r\n",
        "        \"Please sort your idx2word by subword length in decsending manner.\"\r\n",
        "    print(\"The second test passed!\")\r\n",
        "\r\n",
        "    # Third test\r\n",
        "    corpus = ['low'] * 5 + ['lower'] * 2 + ['newest'] * 6 + ['widest'] * 3\r\n",
        "    vocab = set(build_bpe(corpus, max_vocab_size=24))\r\n",
        "    assert vocab > {PAD, UNK, CLS, SEP, MSK, 'est_', 'low', 'newest_', \\\r\n",
        "                    'i', 'e', 'n', 't', 'd', 's', 'o', 'l', 'r', 'w',\r\n",
        "                    WORD_END} and \\\r\n",
        "           \"low_\" not in vocab and \"wi\" not in vocab and \"id\" not in vocab, \\\r\n",
        "        \"Your bpe result does not match expected result\"\r\n",
        "    print(\"The third test passed!\")\r\n",
        "\r\n",
        "    # forth test\r\n",
        "    corpus = ['aaaaaaaaaaaa', 'abababab']\r\n",
        "    vocab = set(build_bpe(corpus, max_vocab_size=13))\r\n",
        "    assert vocab == {PAD, UNK, CLS, SEP, MSK, 'aaaaaaaa', 'aaaa', 'abab', 'aa',\r\n",
        "                     'ab', 'a', 'b', WORD_END}, \\\r\n",
        "        \"Your bpe result does not match expected result\"\r\n",
        "    print(\"The forth test passed!\")\r\n",
        "\r\n",
        "    # fifth test\r\n",
        "    corpus = ['abc', 'bcd']\r\n",
        "    vocab = build_bpe(corpus, max_vocab_size=10000)\r\n",
        "    assert len(vocab) == 15, \\\r\n",
        "        \"Your bpe result does not match expected result\"\r\n",
        "    print(\"The fifth test passed!\")\r\n",
        "\r\n",
        "    print(\"All 5 tests passed!\")\r\n",
        "test_build_bpe()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}