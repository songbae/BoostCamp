{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN visualization \r\n",
    "---\r\n",
    "\r\n",
    "- CNN is a black box \r\n",
    "- \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "### Embedding feature anlaysis 2 \r\n",
    "---\r\n",
    "- Dimensionality reduction \r\n",
    "- t-distributed stochastic neighbor embedding (t-SNE)\r\n",
    "\r\n",
    "- gradient ascent -image synthesis \r\n",
    "\r\n",
    "1. get a prediction score(of the target class) of a dummy image(blank or random initial)\r\n",
    "2. Backpropagate the gradient maximizing the target class score w.r.t the input image \r\n",
    "3. update the current image \r\n",
    "\r\n",
    "### Saliency test 1\r\n",
    "---\r\n",
    "- occlusion map -> 각각의 위치를 test해본다 , prediction scores change accoring to the location of mask \r\n",
    "\r\n",
    "- heatmap representation을 통해서 -> prediction scores drop drastically around the sailent parts ( 히트맵을 통해 어떤 곳이 중요한 keypoint 인지 알 수 있다. )\r\n",
    "\r\n",
    "### Saliency test2 \r\n",
    "---\r\n",
    "\r\n",
    "- via backpropagation \r\n",
    "\r\n",
    "1. get a calss of the target source image \r\n",
    "2. Backpropagate the gradient of the class score w.r.t input domain \r\n",
    "3. Visualize the obtained gradient magnitude map (optionally ,can be accumulated)\r\n",
    "\r\n",
    "#### Recticied unit(backward pass)\r\n",
    "- forward Pass \r\n",
    "- saliency amp \r\n",
    "- Deconvolution \r\n",
    "- \r\n",
    "#### Class activation mapping\r\n",
    "---\r\n",
    "\r\n",
    "class activation mapping (CAM)\r\n",
    "- By visualizing (CAM) , we can interpret why the network classifed the input to that class \r\n",
    "- GAP layer enables localization without supervision \r\n",
    "\r\n",
    "1. Grad_CAM-> 개발 \r\n",
    "2. Get the CAM result without modigying and re_training the original network\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.9950, -1.6064], grad_fn=<MulBackward0>)\n",
      "tensor([300.0000,   0.3000])\n",
      "<MulBackward0 object at 0x0000024AC4BE0670>\n"
     ]
    }
   ],
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "x = torch.randn(2, requires_grad=True)\r\n",
    "y = x * 3\r\n",
    "print(y)\r\n",
    "gradients = torch.tensor([100, 0.1], dtype=torch.float)\r\n",
    "y.backward(gradients)\r\n",
    "print(x.grad)\r\n",
    "print(y.grad_fn)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1846,  0.1836, -0.1686])\n",
      "tensor([ 0.3693,  0.3671, -0.3371], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.7385,  0.7343, -0.6743], grad_fn=<MulBackward0>)\n",
      "tensor([ 1.4771,  1.4685, -1.3486], grad_fn=<MulBackward0>)\n",
      "tensor([ 2.9541,  2.9371, -2.6971], grad_fn=<MulBackward0>)\n",
      "tensor([ 5.9082,  5.8742, -5.3942], grad_fn=<MulBackward0>)\n",
      "tensor([ 11.8165,  11.7483, -10.7884], grad_fn=<MulBackward0>)\n",
      "tensor([ 23.6329,  23.4966, -21.5768], grad_fn=<MulBackward0>)\n",
      "tensor([ 47.2658,  46.9933, -43.1537], grad_fn=<MulBackward0>)\n",
      "tensor([ 94.5317,  93.9865, -86.3074], grad_fn=<MulBackward0>)\n",
      "tensor([ 189.0634,  187.9730, -172.6147], grad_fn=<MulBackward0>)\n",
      "tensor([ 378.1268,  375.9460, -345.2295], grad_fn=<MulBackward0>)\n",
      "tensor([ 756.2535,  751.8920, -690.4590], grad_fn=<MulBackward0>)\n",
      "tensor([ 756.2535,  751.8920, -690.4590], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.variable\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "x = torch.randn(3)\r\n",
    "print(x)\r\n",
    "x = torch.autograd.variable(x, requires_grad=True)\r\n",
    "\r\n",
    "y = x * 2\r\n",
    "print(y)\r\n",
    "while y.data.norm() < 1000:\r\n",
    "  y = y * 2\r\n",
    "  print(y)\r\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\r\n",
    "import wheel\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision\r\n",
    "from torch.autograd import Variable, Function\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "from torchvision import transforms\r\n",
    "import functools\r\n",
    "import urllib\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import sys\r\n",
    "import math\r\n",
    "import random\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from glob import glob\r\n",
    "from scipy.ndimage.filters import gaussian_filter\r\n",
    "from IPython.display import clear_output, Image, display, HTML\r\n",
    "from google.protobuf import text_format\r\n",
    "from io import StringIO\r\n",
    "import PIL.Image\r\n",
    "% matplotlib inline\r\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\r\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a955008dc820c70e8c41cf6f115bde945f96d07d69c96eec2ee76e53bea50083"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}