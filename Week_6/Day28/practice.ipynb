{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "\r\n",
    "class VGG11BackBone(nn.Module):\r\n",
    "  def __init__(self):\r\n",
    "    super(VGG11BackBone, self).__init__()\r\n",
    "\r\n",
    "    self.relu = nn.ReLU(inplace=True)\r\n",
    "\r\n",
    "    # Convolution Feature Extraction Part\r\n",
    "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\r\n",
    "    self.bn1 = nn.BatchNorm2d(64)\r\n",
    "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\r\n",
    "    self.bn2 = nn.BatchNorm2d(128)\r\n",
    "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\r\n",
    "    self.bn3_1 = nn.BatchNorm2d(256)\r\n",
    "    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\r\n",
    "    self.bn3_2 = nn.BatchNorm2d(256)\r\n",
    "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\r\n",
    "    self.bn4_1 = nn.BatchNorm2d(512)\r\n",
    "    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n",
    "    self.bn4_2 = nn.BatchNorm2d(512)\r\n",
    "    self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "    self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n",
    "    self.bn5_1 = nn.BatchNorm2d(512)\r\n",
    "    self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n",
    "    self.bn5_2 = nn.BatchNorm2d(512)\r\n",
    "\r\n",
    "  def forward(self, x):\r\n",
    "    x = self.conv1(x)\r\n",
    "    x = self.bn1(x)\r\n",
    "    x = self.relu(x)\r\n",
    "    x = self.pool1(x)\r\n",
    "\r\n",
    "    x = self.conv2(x)\r\n",
    "    x = self.bn2(x)\r\n",
    "    x = self.relu(x)\r\n",
    "    x = self.pool2(x)\r\n",
    "\r\n",
    "    x = self.conv3_1(x)\r\n",
    "    x = self.bn3_1(x)\r\n",
    "    x = self.relu(x)\r\n",
    "    x = self.conv3_2(x)\r\n",
    "    x = self.bn3_2(x)\r\n",
    "    x = self.relu(x)\r\n",
    "    x = self.pool3(x)\r\n",
    "\r\n",
    "    x = self.conv4_1(x)\r\n",
    "    x = self.bn4_1(x)\r\n",
    "    x = self.relu(x)\r\n",
    "    x = self.conv4_2(x)\r\n",
    "    x = self.bn4_2(x)\r\n",
    "    x = self.relu(x)\r\n",
    "    x = self.pool4(x)\r\n",
    "\r\n",
    "    x = self.conv5_1(x)\r\n",
    "    x = self.bn5_1(x)\r\n",
    "    x = self.relu(x)\r\n",
    "    x = self.conv5_2(x)\r\n",
    "    x = self.bn5_2(x)\r\n",
    "    x = self.relu(x)\r\n",
    "\r\n",
    "    return x\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG11Classification(nn.Module):\r\n",
    "  def __init__(self,num_classes=7):\r\n",
    "    super(VGG11Classification,self).__init__()\r\n",
    "    self.backbone=VGG11BackBone()\r\n",
    "    self.pool5=nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
    "    self.gap=nn.AdaptiveAvgPool2d(1)\r\n",
    "    self.fc_out=nn.Linear(512,num_classes)\r\n",
    "\r\n",
    "  def forward(self,x):\r\n",
    "    x=self.backbone(x)\r\n",
    "    x=self.pool5(x)\r\n",
    "    x=self.gap(x)\r\n",
    "    x=torch.flatten(x)\r\n",
    "    x=self.fc_out(x)\r\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG11Segmentation(nn.Module):\r\n",
    "  def __init(self,num_classes=7):\r\n",
    "    super(VGG11Segmentation,self).__init__()\r\n",
    "    self.backbone=VGG11BackBone()\r\n",
    "    self.conv_out=nn.Conv2d(512,num_classes,kernel_size=1)\r\n",
    "    self.upsample=nn.Upsample(scale_factor=16, mode='bilinear')\r\n",
    "\r\n",
    "  def forward(self,x):\r\n",
    "    x=self.backbone(x)\r\n",
    "    x=self.conv_out(x)\r\n",
    "    x=self.upsample(x)\r\n",
    "    return x\r\n",
    "  \r\n",
    "  def copy_last_layer(self,fc_out):\r\n",
    "    param=fc_out.weight.reshape(7,512,1,1)\r\n",
    "    self.conv_out.weight=nn.Parameter(param)\r\n",
    "    return \r\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7])\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'VGG11Segmentation' object has no attribute 'backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f144aedc3946>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodelS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVGG11Segmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodelS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-427848647007>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m    779\u001b[0m             type(self).__name__, name))\n\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleAttributeError\u001b[0m: 'VGG11Segmentation' object has no attribute 'backbone'"
     ]
    }
   ],
   "source": [
    "\r\n",
    "test_input=torch.randn((1,3,224,224))\r\n",
    "modelC=VGG11Classification()\r\n",
    "out=modelC(test_input)\r\n",
    "print(out.shape)\r\n",
    "modelS=VGG11Segmentation()\r\n",
    "out=modelS(test_input)\r\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \r\n",
    "from torchvision import transforms\r\n",
    "from torch.utils.data import Dataset,DataLoader \r\n",
    "import os \r\n",
    "import cv2 \r\n",
    "import numpy as np \r\n",
    "from glob import glob\r\n",
    "\r\n",
    "class MaskDataset(Dataset):\r\n",
    "  def __init__(self,data_root,input_size=224, transform=None):\r\n",
    "    super(MaskDataset,self).__init__()\r\n",
    "\r\n",
    "    self.img_list=glob(os.path.join(data_root,'*.jpg'))\r\n",
    "    self.len=len(self.img_list)\r\n",
    "    self.input_size=input_size\r\n",
    "    self.transforms= transform \r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "      img_path=self.img_list[index]\r\n",
    "\r\n",
    "      # image loading \r\n",
    "      img=cv2.imread(img_path)\r\n",
    "      img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
    "      img=img/255. # 이건 이지미의 값들을 전부 255 의 값으로 나눠서 정규화 시킨 모양 \r\n",
    "      if self.transform:\r\n",
    "        img =self.transform(img )\r\n",
    "\r\n",
    "      # Ground Truth \r\n",
    "      label=self._get_class_idx_from_img_name(img_path)\r\n",
    "\r\n",
    "      return img, label \r\n",
    "    def __len__(self):\r\n",
    "      return self.len \r\n",
    "\r\n",
    "    def _get_class_idx_from_img_name(self,img_path):\r\n",
    "      img_name=os.path.basename(img_path)\r\n",
    "      \r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-c8cfc8e9fcd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_root\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'./model.pth'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodelC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVGG11Classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodelC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_root\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m transform=transforms.Compose([\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model.pth'"
     ]
    }
   ],
   "source": [
    "## Model Loading \r\n",
    "model_root ='./model.pth'\r\n",
    "modelC=VGG11Classification()\r\n",
    "modelC.load_state_dict(torch.load(model_root))\r\n",
    "input_size=224\r\n",
    "transform=transforms.Compose([\r\n",
    "  transforms.ToTensor(),\r\n",
    "  transforms.Resize((224,224)),\r\n",
    "  transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\r\n",
    "])\r\n",
    "batch_size=1\r\n",
    "test_dataset=MaskDataset(data_root,input_size=input_size, transform=transform)\r\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size,suffle=True, pin_memory=True)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\r\n",
    "modelS.cuda().float()\r\n",
    "modeS.eval()\r\n",
    "\r\n",
    "for iter, (img,label) in enumerate(test_loader):\r\n",
    "  img=img.flaot().cuda()\r\n",
    "  # inference for Semantic Segmentation \r\n",
    "  res=modeS(img)[0]\r\n",
    "  heat=res[label[0]]\r\n",
    "  resH="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt \r\n",
    "# def show_landmark(image, landmarks):\r\n",
    "#   plt.imshow(image)\r\n",
    "#   plt.scatter(landmarks[:,0],landmarks[:,1],s=10, marker='.', c='r')\r\n",
    "#   plt.pause(0.001)# 갱신이 되도록 잠시 멈춥니다 \r\n",
    "\r\n",
    "# plt.figure()\r\n",
    "# show_landmarks(io.imread(os.path.join('data/faces/',img_name)),landmarks)\r\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "# class FaceLadnmarksDataset(Dataset):\r\n",
    "#   def __init__(self,csv_file,root_dir,transfrom=None):\r\n",
    "#     self.landmarks_frame=pd.read_csv(csv_file)\r\n",
    "#     self.root_dir=root_dir\r\n",
    "#     self.transform=transform \r\n",
    "\r\n",
    "#   def __len__(self):\r\n",
    "#     return len(self.landmarks_frame)\r\n",
    "\r\n",
    "#   def __getitem__(self, idx):\r\n",
    "#     if torch.is_tensor(idx):\r\n",
    "#       idx=idx.tolist()\r\n",
    "    \r\n",
    "#     img_name=os.path.join(self.root_dir,self.landmarks_frame.iloc[idx,0])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Rescale(object):\r\n",
    "\r\n",
    "#   def __init__(self,output_size):\r\n",
    "#     assert isinstance(output_size, (int,tuple))\r\n",
    "#     self.output_size=output_size\r\n",
    "\r\n",
    "#   def __call__(self,sample):\r\n",
    "#     image,landmarks=sample['image'],sample['landmarks']\r\n",
    "\r\n",
    "#     h,w = image.shape[:2]\r\n",
    "\r\n",
    "#     if isinstance(self.output_size, int):\r\n",
    "#       if h>w: # 높이가 w보다 크다면 h 와 w의 크기를 갖게 맞춰준다 즉 크롭된 영상은 정사각형스타일이다. \r\n",
    "        \r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a955008dc820c70e8c41cf6f115bde945f96d07d69c96eec2ee76e53bea50083"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}