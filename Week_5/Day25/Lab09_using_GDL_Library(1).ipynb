{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab09. using GDL Library(1)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW394yGalVGt"
      },
      "source": [
        "# 실습 09. \r\n",
        "\r\n",
        "**from dgl.nn import SAGEConv** 를 활용하여 GraphSAGE 모델을 구현하고 학습시켜보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqrq01umVpvh"
      },
      "source": [
        "!pip install dgl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dgl in c:\\users\\songbae\\anaconda3\\lib\\site-packages (0.6.0)\nRequirement already satisfied: networkx>=2.1 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from dgl) (2.5)\nRequirement already satisfied: scipy>=1.1.0 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from dgl) (1.5.2)\nRequirement already satisfied: numpy>=1.14.0 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from dgl) (1.19.2)\nRequirement already satisfied: requests>=2.19.0 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from dgl) (2.24.0)\nRequirement already satisfied: decorator>=4.3.0 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from networkx>=2.1->dgl) (4.4.2)\nRequirement already satisfied: idna<3,>=2.5 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (1.25.11)\nRequirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\songbae\\anaconda3\\lib\\site-packages (from requests>=2.19.0->dgl) (2020.6.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2yfG0Aj_oej"
      },
      "source": [
        "import numpy as np\r\n",
        "import time\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import dgl\r\n",
        "from dgl.data import CoraGraphDataset\r\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y56IETYS-lqg"
      },
      "source": [
        "# 하이퍼파라미터 초기화\r\n",
        "dropoutProb = 0.5\r\n",
        "learningRate = 1e-2\r\n",
        "numEpochs = 50\r\n",
        "numHiddenDim = 128\r\n",
        "numLayers = 2\r\n",
        "weightDecay = 5e-4\r\n",
        "aggregatorType = \"gcn\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ux5YI1pEh0"
      },
      "source": [
        "'''\r\n",
        "    Cora 데이터셋은 2708개의 논문(노드), 10556개의 인용관계(엣지)로 이루어졌습니다. \r\n",
        "    NumFeat은 각 노드를 나타내는 특성을 말합니다. \r\n",
        "    Cora 데이터셋은 각 노드가 1433개의 특성을 가지고, 개개의 특성은 '1'혹은 '0'으로 나타내어지며 특정 단어의 논문 등장 여부를 나타냅니다.\r\n",
        "    즉, 2708개의 논문에서 특정 단어 1433개를 뽑아서, 1433개의 단어의 등장 여부를 통해 각 노드를 표현합니다.\r\n",
        "    \r\n",
        "    노드의 라벨은 총 7개가 존재하고, 각 라벨은 논문의 주제를 나타냅니다\r\n",
        "    [Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory]\r\n",
        "\r\n",
        "    2708개의 노드 중, 학습에는 140개의 노드를 사용하고 모델을 테스트하는 데에는 1000개를 사용합니다.\r\n",
        "    본 실습에서는 Validation을 진행하지않습니다.\r\n",
        "\r\n",
        "    요약하자면, 앞서 학습시킬 모델은 Cora 데이터셋의 \r\n",
        "    [논문 내 등장 단어들, 논문들 사이의 인용관계]를 활용하여 논문의 주제를 예측하는 모델입니다.\r\n",
        "'''\r\n",
        "\r\n",
        "# Cora Graph Dataset 불러오기\r\n",
        "G = CoraGraphDataset()\r\n",
        "numClasses = G.num_classes\r\n",
        "\r\n",
        "G = G[0]\r\n",
        "# 노드들의 feauture & feature의 차원\r\n",
        "features = G.ndata['feat']\r\n",
        "inputFeatureDim = features.shape[1]\r\n",
        "\r\n",
        "# 각 노드들의 실제 라벨\r\n",
        "labels = G.ndata['label']\r\n",
        "\r\n",
        "# 학습/테스트에 사용할 노드들에 대한 표시\r\n",
        "trainMask = G.ndata['train_mask']        \r\n",
        "testMask = G.ndata['test_mask']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading C:\\Users\\songbae\\.dgl\\cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "Extracting file to C:\\Users\\songbae\\.dgl\\cora_v2\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfp4dy6TpEfl"
      },
      "source": [
        "# 모델 학습 결과를 평가할 함수\r\n",
        "def evaluateTrain(model, features, labels, mask):\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        logits = model(features)\r\n",
        "        logits = logits[mask]\r\n",
        "        labels = labels[mask]\r\n",
        "        _, indices = torch.max(logits, dim=1)\r\n",
        "        correct = torch.sum(indices == labels)\r\n",
        "        return correct.item() * 1.0 / len(labels)\r\n",
        "\r\n",
        "def evaluateTest(model, features, labels, mask):\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        logits = model(features)\r\n",
        "        logits = logits[mask]\r\n",
        "        labels = labels[mask]\r\n",
        "        _, indices = torch.max(logits, dim=1)\r\n",
        "        macro_f1 = f1_score(labels, indices, average = 'macro')\r\n",
        "        correct = torch.sum(indices == labels)\r\n",
        "        return correct.item() * 1.0 / len(labels), macro_f1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVMJ1qDS84fI"
      },
      "source": [
        "def train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs):\r\n",
        "    executionTime=[]\r\n",
        "    \r\n",
        "    for epoch in range(numEpochs):\r\n",
        "        model.train()\r\n",
        "\r\n",
        "        startTime = time.time()\r\n",
        "            \r\n",
        "        logits = model(features)                                    # 포워딩\r\n",
        "        loss = lossFunction(logits[trainMask], labels[trainMask])   # 모델의 예측값과 실제 라벨을 비교하여 loss 값 계산\r\n",
        "\r\n",
        "        optimizer.zero_grad()                                       \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        executionTime.append(time.time() - startTime)\r\n",
        "\r\n",
        "        acc = evaluateTrain(model, features, labels, trainMask)\r\n",
        "\r\n",
        "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f}\".format(epoch, np.mean(executionTime), loss.item(), acc))\r\n",
        "\r\n",
        "def test(model, feautures, labels, testMask):\r\n",
        "    acc, macro_f1 = evaluateTest(model, features, labels, testMask)\r\n",
        "    print(\"Test Accuracy {:.4f}\".format(acc))\r\n",
        "    print(\"Test macro-f1 {:.4f}\".format(macro_f1))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYuLoRxfYQyY"
      },
      "source": [
        "# Define GraphSage architecture\n",
        "# 기존에 구현되어 있는 SAGEConv 모듈을 불러와서, SAGEConv로 이루어진 GraphSAGE 모델을 구축한다.\n",
        "from dgl.nn.pytorch.conv import SAGEConv\n",
        "class GraphSAGE(nn.Module):\n",
        "    '''\n",
        "        graph               : 학습할 그래프\n",
        "        inFeatDim           : 데이터의 feature의 차원\n",
        "        numHiddenDim        : 모델의 hidden 차원\n",
        "        numClasses          : 예측할 라벨의 경우의 수\n",
        "        numLayers           : 인풋, 아웃풋 레이어를 제외하고 중간 레이어의 갯수\n",
        "        activationFunction  : 활성화 함수의 종류\n",
        "        dropoutProb         : 드롭아웃 할 확률\n",
        "        aggregatorType      : [mean, gcn, pool (for max), lstm]\n",
        "    '''\n",
        "    '''\n",
        "        SAGEConv(inputFeatDim, outputFeatDim, aggregatorType, dropoutProb, activationFunction)와 같은 형식으로 모듈 생성\n",
        "    '''\n",
        "    def __init__(self,graph, inFeatDim, numHiddenDim, numClasses, numLayers, activationFunction, dropoutProb, aggregatorType):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.graph = graph\n",
        "\n",
        "        # 인풋 레이어\n",
        "        self.layers.append(SAGEConv(inFeatDim,numHiddenDim,aggregatorType,dropoutProb,activationFunction))\n",
        "       \n",
        "        # 히든 레이어\n",
        "        for i in range(numLayers):\n",
        "            self.layers.append(SAGEConv(numHiddenDim,numHiddenDim,aggregatorType,dropoutProb,activationFunction))\n",
        "        \n",
        "        # 출력 레이어\n",
        "        self.layers.append(SAGEConv(numHiddenDim, numClasses, aggregatorType, dropoutProb, activation=None))\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features\n",
        "        for layer in self.layers:\n",
        "            x = layer(self.graph, x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKeX9AdBpJaN"
      },
      "source": [
        "# 모델 생성\n",
        "model = GraphSAGE(G, inputFeatureDim, numHiddenDim, numClasses, numLayers, F.relu, dropoutProb, aggregatorType)\n",
        "print(model)\n",
        "lossFunction = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# 옵티마이저 초기화\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphSAGE(\n  (layers): ModuleList(\n    (0): SAGEConv(\n      (feat_drop): Dropout(p=0.5, inplace=False)\n      (fc_neigh): Linear(in_features=1433, out_features=128, bias=True)\n    )\n    (1): SAGEConv(\n      (feat_drop): Dropout(p=0.5, inplace=False)\n      (fc_neigh): Linear(in_features=128, out_features=128, bias=True)\n    )\n    (2): SAGEConv(\n      (feat_drop): Dropout(p=0.5, inplace=False)\n      (fc_neigh): Linear(in_features=128, out_features=128, bias=True)\n    )\n    (3): SAGEConv(\n      (feat_drop): Dropout(p=0.5, inplace=False)\n      (fc_neigh): Linear(in_features=128, out_features=7, bias=True)\n    )\n  )\n)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY9nnzs1pJcb"
      },
      "source": [
        "train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00000 | Time(s) 0.1756 | Loss 1.9459 | Accuracy 0.1429\n",
            "Epoch 00001 | Time(s) 0.1703 | Loss 2.0015 | Accuracy 0.2286\n",
            "Epoch 00002 | Time(s) 0.1716 | Loss 1.7764 | Accuracy 0.5929\n",
            "Epoch 00003 | Time(s) 0.1714 | Loss 1.6911 | Accuracy 0.7214\n",
            "Epoch 00004 | Time(s) 0.1708 | Loss 1.4340 | Accuracy 0.9071\n",
            "Epoch 00005 | Time(s) 0.1711 | Loss 1.1815 | Accuracy 0.8643\n",
            "Epoch 00006 | Time(s) 0.1713 | Loss 0.9026 | Accuracy 0.8786\n",
            "Epoch 00007 | Time(s) 0.1722 | Loss 0.6705 | Accuracy 0.8929\n",
            "Epoch 00008 | Time(s) 0.1730 | Loss 0.4923 | Accuracy 0.9286\n",
            "Epoch 00009 | Time(s) 0.1756 | Loss 0.2965 | Accuracy 0.9643\n",
            "Epoch 00010 | Time(s) 0.1772 | Loss 0.2359 | Accuracy 0.9643\n",
            "Epoch 00011 | Time(s) 0.1788 | Loss 0.1902 | Accuracy 0.9714\n",
            "Epoch 00012 | Time(s) 0.1803 | Loss 0.1133 | Accuracy 0.9857\n",
            "Epoch 00013 | Time(s) 0.1822 | Loss 0.0908 | Accuracy 0.9929\n",
            "Epoch 00014 | Time(s) 0.1837 | Loss 0.0825 | Accuracy 0.9857\n",
            "Epoch 00015 | Time(s) 0.1852 | Loss 0.0872 | Accuracy 0.9857\n",
            "Epoch 00016 | Time(s) 0.1863 | Loss 0.0899 | Accuracy 0.9929\n",
            "Epoch 00017 | Time(s) 0.1875 | Loss 0.0403 | Accuracy 0.9857\n",
            "Epoch 00018 | Time(s) 0.1888 | Loss 0.0531 | Accuracy 0.9857\n",
            "Epoch 00019 | Time(s) 0.1898 | Loss 0.0495 | Accuracy 1.0000\n",
            "Epoch 00020 | Time(s) 0.1907 | Loss 0.0507 | Accuracy 0.9857\n",
            "Epoch 00021 | Time(s) 0.1914 | Loss 0.0517 | Accuracy 0.9929\n",
            "Epoch 00022 | Time(s) 0.1923 | Loss 0.0609 | Accuracy 0.9786\n",
            "Epoch 00023 | Time(s) 0.1930 | Loss 0.0824 | Accuracy 0.9929\n",
            "Epoch 00024 | Time(s) 0.1936 | Loss 0.0173 | Accuracy 1.0000\n",
            "Epoch 00025 | Time(s) 0.1940 | Loss 0.0144 | Accuracy 1.0000\n",
            "Epoch 00026 | Time(s) 0.1951 | Loss 0.0213 | Accuracy 1.0000\n",
            "Epoch 00027 | Time(s) 0.1955 | Loss 0.0602 | Accuracy 0.9857\n",
            "Epoch 00028 | Time(s) 0.1960 | Loss 0.0296 | Accuracy 1.0000\n",
            "Epoch 00029 | Time(s) 0.1963 | Loss 0.0265 | Accuracy 1.0000\n",
            "Epoch 00030 | Time(s) 0.1970 | Loss 0.0172 | Accuracy 0.9929\n",
            "Epoch 00031 | Time(s) 0.1975 | Loss 0.0501 | Accuracy 0.9929\n",
            "Epoch 00032 | Time(s) 0.1981 | Loss 0.0294 | Accuracy 1.0000\n",
            "Epoch 00033 | Time(s) 0.1986 | Loss 0.0111 | Accuracy 1.0000\n",
            "Epoch 00034 | Time(s) 0.1990 | Loss 0.0093 | Accuracy 1.0000\n",
            "Epoch 00035 | Time(s) 0.1993 | Loss 0.0125 | Accuracy 1.0000\n",
            "Epoch 00036 | Time(s) 0.2000 | Loss 0.0111 | Accuracy 1.0000\n",
            "Epoch 00037 | Time(s) 0.2002 | Loss 0.0483 | Accuracy 1.0000\n",
            "Epoch 00038 | Time(s) 0.2006 | Loss 0.0142 | Accuracy 1.0000\n",
            "Epoch 00039 | Time(s) 0.2011 | Loss 0.0267 | Accuracy 1.0000\n",
            "Epoch 00040 | Time(s) 0.2013 | Loss 0.0194 | Accuracy 1.0000\n",
            "Epoch 00041 | Time(s) 0.2015 | Loss 0.0388 | Accuracy 1.0000\n",
            "Epoch 00042 | Time(s) 0.2018 | Loss 0.0179 | Accuracy 1.0000\n",
            "Epoch 00043 | Time(s) 0.2020 | Loss 0.0359 | Accuracy 1.0000\n",
            "Epoch 00044 | Time(s) 0.2026 | Loss 0.0238 | Accuracy 0.9929\n",
            "Epoch 00045 | Time(s) 0.2028 | Loss 0.0812 | Accuracy 0.9929\n",
            "Epoch 00046 | Time(s) 0.2032 | Loss 0.0380 | Accuracy 1.0000\n",
            "Epoch 00047 | Time(s) 0.2034 | Loss 0.0565 | Accuracy 1.0000\n",
            "Epoch 00048 | Time(s) 0.2036 | Loss 0.1169 | Accuracy 1.0000\n",
            "Epoch 00049 | Time(s) 0.2038 | Loss 0.0390 | Accuracy 0.9929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-swaKM7E-KiY"
      },
      "source": [
        "test(model, features, labels, testMask)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy 0.7870\nTest macro-f1 0.7746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5E2HkTNA6DR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}