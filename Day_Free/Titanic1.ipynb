{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a955008dc820c70e8c41cf6f115bde945f96d07d69c96eec2ee76e53bea50083"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(891, 12) (418, 11) (418, 2)\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\nPassengerId      int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\nPassengerId    int64\nSurvived       int64\ndtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import Counter \n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train=pd.read_csv('./data/train.csv')\n",
    "df_test=pd.read_csv('./data/test.csv')\n",
    "df_sub=pd.read_csv('./data/gender_submission.csv')\n",
    "print(df_train.shape,df_test.shape,df_sub.shape)\n",
    "print(df_train.dtypes,end='\\n')\n",
    "print(df_test.dtypes,end='\\n')\n",
    "print(df_sub.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n0            1         0       3  ...   7.2500   NaN         S\n1            2         1       1  ...  71.2833   C85         C\n2            3         1       3  ...   7.9250   NaN         S\n3            4         1       1  ...  53.1000  C123         S\n4            5         0       3  ...   8.0500   NaN         S\n\n[5 rows x 12 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nNone\n['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "print(df_train.head())\n",
    "print(df_train.info())\n",
    "print(df_train['Embarked'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\n",
    "df_test.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\n",
    "\n",
    "sex=pd.get_dummies(df_train['Sex'],drop_first=True)\n",
    "embark=pd.get_dummies(df_train['Embarked'],drop_first=True)\n",
    "df_train=pd.concat([df_train,sex,embark],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\ndtype: object\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"['Sex' 'Embarked'] not found in axis\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-a395c1af02b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sex'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Embarked'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4161\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4162\u001b[0m         \"\"\"\n\u001b[1;32m-> 4163\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4164\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4165\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3919\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3920\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3921\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3922\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5281\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5282\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5283\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Sex' 'Embarked'] not found in axis\""
     ]
    }
   ],
   "source": [
    "print(df_train.dtypes)\n",
    "df_train.drop(['Sex','Embarked'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Pclass  Survived\n0       1  0.629630\n1       2  0.472826\n2       3  0.242363\n        Survived\nPclass          \n1       0.629630\n2       0.472826\n3       0.242363\n"
     ]
    }
   ],
   "source": [
    "print(df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False))\n",
    "print(df_train[['Pclass','Survived']].groupby(['Pclass']).mean().sort_values(by='Pclass'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = pd.get_dummies(df_test['Sex'],drop_first=True)\n",
    "embark = pd.get_dummies(df_test['Embarked'],drop_first=True)\n",
    "df_test = pd.concat([df_test,sex,embark],axis=1)\n",
    "\n",
    "df_test.drop(['Sex','Embarked'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            0         1         2         3         4         5         6         7         8         9\n0   -1.730108 -0.789272  0.827377 -0.592481  0.432793 -0.473674 -0.502445  0.737695 -0.307562  0.619306\n1   -1.726220  1.266990 -1.566107  0.638789  0.432793 -0.473674  0.786845 -1.355574 -0.307562 -1.614710\n2   -1.722332  1.266990  0.827377 -0.284663 -0.474545 -0.473674 -0.488854 -1.355574 -0.307562  0.619306\n3   -1.718444  1.266990 -1.566107  0.407926  0.432793 -0.473674  0.420730 -1.355574 -0.307562  0.619306\n4   -1.714556 -0.789272  0.827377  0.407926 -0.474545 -0.473674 -0.486337  0.737695 -0.307562  0.619306\n..        ...       ...       ...       ...       ...       ...       ...       ...       ...       ...\n886  1.714556 -0.789272 -0.369365 -0.207709 -0.474545 -0.473674 -0.386671  0.737695 -0.307562  0.619306\n887  1.718444  1.266990 -1.566107 -0.823344 -0.474545 -0.473674 -0.044381 -1.355574 -0.307562  0.619306\n888  1.722332 -0.789272  0.827377  0.000000  0.432793  2.008933 -0.176263 -1.355574 -0.307562  0.619306\n889  1.726220  1.266990 -1.566107 -0.284663 -0.474545 -0.473674 -0.044381  0.737695 -0.307562 -1.614710\n890  1.730108 -0.789272  0.827377  0.177063 -0.474545 -0.473674 -0.492378  0.737695  3.251373 -1.614710\n\n[891 rows x 10 columns] 0    float64\n1    float64\n2    float64\n3    float64\n4    float64\n5    float64\n6    float64\n7    float64\n8    float64\n9    float64\ndtype: object\n            0         1         2         3         4         5         6         7         8\n0   -1.727912  0.873482  0.334993 -0.499470 -0.400248 -0.498407  0.755929  2.843757 -1.350676\n1   -1.719625  0.873482  1.325530  0.616992 -0.400248 -0.513274 -1.322876 -0.351647  0.740370\n2   -1.711337 -0.315819  2.514175 -0.499470 -0.400248 -0.465088  0.755929  2.843757 -1.350676\n3   -1.703050  0.873482 -0.259330 -0.499470 -0.400248 -0.483466  0.755929 -0.351647  0.740370\n4   -1.694763  0.873482 -0.655545  0.616992  0.619896 -0.418471 -1.322876 -0.351647  0.740370\n..        ...       ...       ...       ...       ...       ...       ...       ...       ...\n413  1.694763  0.873482  0.000000 -0.499470 -0.400248 -0.494448  0.755929 -0.351647  0.740370\n414  1.703050 -1.505120  0.691586 -0.499470 -0.400248  1.313753 -1.322876 -0.351647 -1.350676\n415  1.711337  0.873482  0.651965 -0.499470 -0.400248 -0.508792  0.755929 -0.351647  0.740370\n416  1.719625  0.873482  0.000000 -0.499470 -0.400248 -0.494448  0.755929 -0.351647  0.740370\n417  1.727912  0.873482  0.000000  0.616992  0.619896 -0.237906  0.755929 -0.351647 -1.350676\n\n[418 rows x 9 columns]\nIndex(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'male', 'Q', 'S'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df_train.fillna(df_train.mean(),inplace=True)\n",
    "df_test.fillna(df_test.mean(), inplace=True)\n",
    "\n",
    "Scaler1=StandardScaler()\n",
    "Scaler2=StandardScaler()\n",
    "\n",
    "train_columns=df_train.columns\n",
    "test_columns= df_test.columns \n",
    "\n",
    "df_train=pd.DataFrame(Scaler1.fit_transform(df_train))\n",
    "df_test=pd.DataFrame(Scaler2.fit_transform(df_test))\n",
    "print(df_train,df_train.dtypes)\n",
    "print(df_test)\n",
    "print(train_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns=train_columns\n",
    "df_test.columns=test_columns\n",
    "features=df_train.iloc[:,2:].columns.tolist()\n",
    "target=df_train.loc[:,'Survived'].name\n",
    "\n",
    "X_train=df_train.iloc[:,2:].values\n",
    "y_train=df_train.loc[:,'Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (fc1): Linear(in_features=8, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=512, bias=True)\n  (fc3): Linear(in_features=512, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1=nn.Linear(8,512 )\n",
    "        self.fc2=nn.Linear(512,512)\n",
    "        self.fc3=nn.Linear(512,2)\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.dropout(x)\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.dropout(x)\n",
    "        x=self.fc3(x)\n",
    "        return x \n",
    "model=Net()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "validation loss decreasedinf->0.6050595198951318\n",
      "Epoch:1\t Train_loss:0.6050595198951318\t Train Accuracy:0.0\n",
      "validation loss decreased0.6050595198951318->0.5826017708260696\n",
      "validation loss decreased0.5826017708260696->0.564212450391859\n",
      "validation loss decreased0.564212450391859->0.5508869436955287\n",
      "validation loss decreased0.5508869436955287->0.5348290153656182\n",
      "validation loss decreased0.5348290153656182->0.525316635360478\n",
      "validation loss decreased0.525316635360478->0.5135092541732529\n",
      "validation loss decreased0.5135092541732529->0.50349576703863\n",
      "validation loss decreased0.50349576703863->0.49771515643681563\n",
      "validation loss decreased0.49771515643681563->0.4840710274623309\n",
      "validation loss decreased0.4840710274623309->0.4783006760800512\n",
      "validation loss decreased0.4783006760800512->0.465274676580591\n",
      "validation loss decreased0.465274676580591->0.4619725500927444\n",
      "validation loss decreased0.4619725500927444->0.45290937897726724\n",
      "validation loss decreased0.45290937897726724->0.4510903469828194\n",
      "validation loss decreased0.4510903469828194->0.4434328459319874\n",
      "validation loss decreased0.4434328459319874->0.4375644699249267\n",
      "validation loss decreased0.4375644699249267->0.43555610519907534\n",
      "validation loss decreased0.43555610519907534->0.4296564233595436\n",
      "validation loss decreased0.4296564233595436->0.42741167924089\n",
      "validation loss decreased0.42741167924089->0.4228225151740305\n",
      "validation loss decreased0.4228225151740305->0.41973101762721426\n",
      "validation loss decreased0.41973101762721426->0.41514337784945843\n",
      "validation loss decreased0.41514337784945843->0.4103508395726666\n",
      "validation loss decreased0.4103508395726666->0.40636129844010027\n",
      "validation loss decreased0.40636129844010027->0.4055966128991222\n",
      "validation loss decreased0.4055966128991222->0.40224493486334556\n",
      "validation loss decreased0.40224493486334556->0.4010204315023061\n",
      "validation loss decreased0.4010204315023061->0.39999972385186316\n",
      "validation loss decreased0.39999972385186316->0.3957146989353982\n",
      "validation loss decreased0.3957146989353982->0.39382363897468636\n",
      "validation loss decreased0.39382363897468636->0.39237901888714577\n",
      "validation loss decreased0.39237901888714577->0.3915964325907079\n",
      "validation loss decreased0.3915964325907079->0.3912396014830082\n",
      "validation loss decreased0.3912396014830082->0.38754902662717494\n",
      "validation loss decreased0.38754902662717494->0.3870415806456122\n",
      "validation loss decreased0.3870415806456122->0.3838977048481992\n",
      "validation loss decreased0.3838977048481992->0.3828101644288639\n",
      "validation loss decreased0.3828101644288639->0.3809556790796345\n",
      "validation loss decreased0.3809556790796345->0.3808797647124924\n",
      "validation loss decreased0.3808797647124924->0.380857728151267\n",
      "validation loss decreased0.380857728151267->0.3792048349103852\n",
      "validation loss decreased0.3792048349103852->0.3768469825352574\n",
      "validation loss decreased0.3768469825352574->0.3733444642028351\n",
      "validation loss decreased0.3733444642028351->0.37010922302782456\n",
      "validation loss decreased0.37010922302782456->0.36811537053855503\n",
      "validation loss decreased0.36811537053855503->0.3648295560545465\n",
      "validation loss decreased0.3648295560545465->0.3643902276944557\n",
      "validation loss decreased0.3643902276944557->0.36322375429989906\n",
      "validation loss decreased0.36322375429989906->0.36034556454563654\n",
      "validation loss decreased0.36034556454563654->0.3573058262181308\n",
      "validation loss decreased0.3573058262181308->0.35583752138065877\n",
      "validation loss decreased0.35583752138065877->0.35533641584507597\n",
      "validation loss decreased0.35533641584507597->0.3533798026870144\n",
      "validation loss decreased0.3533798026870144->0.3510560755803859\n",
      "Epoch:201\t Train_loss:0.3510560755803859\t Train Accuracy:0.0\n",
      "validation loss decreased0.3510560755803859->0.3503330134135646\n",
      "validation loss decreased0.3503330134135646->0.3490008785234818\n",
      "validation loss decreased0.3490008785234818->0.3442717589538242\n",
      "validation loss decreased0.3442717589538242->0.34365328820245694\n",
      "validation loss decreased0.34365328820245694->0.34347334338819335\n",
      "validation loss decreased0.34347334338819335->0.34035161625738664\n",
      "validation loss decreased0.34035161625738664->0.3390723361546433\n",
      "validation loss decreased0.3390723361546433->0.3381426376300186\n",
      "Epoch:401\t Train_loss:0.34662217053804456\t Train Accuracy:0.0\n",
      "validation loss decreased0.3381426376300186->0.3370009770628151\n",
      "validation loss decreased0.3370009770628151->0.3352767387078931\n",
      "validation loss decreased0.3352767387078931->0.33374305899103246\n",
      "validation loss decreased0.33374305899103246->0.32994687963824715\n",
      "Train End\n"
     ]
    }
   ],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-2)\n",
    "\n",
    "batch_size=128\n",
    "n_epochs=500 \n",
    "batch_no=len(X_train)//batch_size\n",
    "\n",
    "train_loss=0\n",
    "train_loss_min=np.Inf\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(batch_no):\n",
    "        start=i*batch_size\n",
    "        end=start+batch_size\n",
    "\n",
    "        x_var=Variable(torch.FloatTensor(X_train[start:end]))\n",
    "        y_var=Variable(torch.LongTensor(y_train[start:end]))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output=model(x_var)\n",
    "        loss=criterion(output,y_var)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        values,labels=torch.max(output,1)\n",
    "        num_right=np.sum(labels.data.numpy()==y_train[start:end])\n",
    "        train_loss+=loss.item()*batch_size\n",
    "    \n",
    "    train_loss=train_loss/len(X_train)\n",
    "    if train_loss<=train_loss_min:\n",
    "        print(f\"validation loss decreased{train_loss_min}->{train_loss}\")\n",
    "        torch.save(model.state_dict(),'model.pt')\n",
    "        train_loss_min=train_loss\n",
    "\n",
    "    if epoch%200==0:\n",
    "        print(f\"Epoch:{epoch+1}\\t Train_loss:{train_loss}\\t Train Accuracy:{num_right/len(y_train[start:end])}\")\n",
    "\n",
    "print('Train End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "###predictions \n",
    "\n",
    "X_test=df_test.iloc[:,1:].values\n",
    "X_test_var=Variable(torch.FloatTensor(X_test),requires_grad=False)\n",
    "with torch.no_grad():\n",
    "    test_result=model(X_test_var)\n",
    "values,labels= torch.max(test_result,1)\n",
    "survived=labels.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'PassengerId': df_sub['PassengerId'], 'Survived': survived})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ]
}