{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "N,D_in,H,D_out = 64,1000,100,10 \n",
    "x= np.random.randn(N, D_in)\n",
    "y=np.random.randn(N,D_out)\n",
    "w1= np.random.randn(D_in,H)\n",
    "w2=np.random.randn(H,D_out)\n",
    "LR=1e-6\n",
    "\n",
    "\n",
    "for t in range(400):\n",
    "    # 순전파 단계 \n",
    "    h=x.dot(w1)\n",
    "    h_relu=np.maximum(h,0)\n",
    "    y_pred=h_relu.dot(w2)\n",
    "\n",
    "    # loss 를 계산하고 출력한다.\n",
    "    loss= np.square(y_pred-y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    #손실에 따른 w1,w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred=2.0*(y_pred-y)\n",
    "    grad_w2=h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu=grad_y_pred.dot(w2.T)\n",
    "    grad_h=grad_h_relu.copy()\n",
    "    grad_h[h<0]=0\n",
    "    grad_w1=x.T.dot(grad_h)\n",
    "\n",
    "    #가중치를 갱신합니다 \n",
    "    w1-=LR*grad_w1\n",
    "    w2-=LR*grad_w2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "dtype=torch.float\n",
    "device=torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "print(device)\n",
    "N,D_in, H,D_out =64,1000,100,10\n",
    "x=torch.randn(N,D_in,device=device,dtype=dtype)\n",
    "y=torch.randn(N,D_out,device=device,dtype=dtype)\n",
    "# 무작위로 가중치를 초기화하기\n",
    "w1=torch.randn(D_in,H,device=device,dtype=dtype)\n",
    "w2=torch.randn(H,D_out,device=device,dtype=dtype)\n",
    "LR=1e-5/11\n",
    "print(x.shape,w1.shape,y.shape,w2.shape)\n",
    "for t in range(500):\n",
    "    h=x.matmul(w1)\n",
    "    h_relu=h.clamp(min=0)\n",
    "    y_pred=h_relu.matmul(w2)\n",
    "\n",
    "    # loss 를 계산하고 출력한다\n",
    "    loss=(y_pred-y).pow(2).sum().item()\n",
    "    if t%10==0:\n",
    "        print(t,loss)\n",
    "    # 손실에 따른 w1,w2의 변화도를 계산하고 역전파합니다\n",
    "    grad_y_pred=2.0*(y_pred-y)\n",
    "    grad_w2=h_relu.t().matmul(grad_y_pred)\n",
    "    grad_h_relu=grad_y_pred.matmul(w2.t())\n",
    "    grad_h=grad_h_relu.clone()\n",
    "    grad_h[h<0]=0\n",
    "    grad_w1=x.t().matmul(grad_h)\n",
    "\n",
    "    # 경사하강법을 사용하여 가중치를 갱신한다.\n",
    "    w1-=LR*grad_w1\n",
    "    w2-=LR*grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "dtype=torch.float\n",
    "device=torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "print(device)\n",
    "N,D_in, H,D_out =64,1000,100,10\n",
    "x=torch.randn(N,D_in,device=device,dtype=dtype)\n",
    "y=torch.randn(N,D_out,device=device,dtype=dtype)\n",
    "# 무작위로 가중치를 초기화하기\n",
    "w1=torch.randn(D_in,H,device=device,dtype=dtype,requires_grad=True)\n",
    "w2=torch.randn(H,D_out,device=device,dtype=dtype,requires_grad=True)\n",
    "LR=1e-5/11\n",
    "print(x.shape,w1.shape,y.shape,w2.shape)\n",
    "for t in range(500):\n",
    "    # h=x.matmul(w1)\n",
    "    # h_relu=h.clamp(min=0)\n",
    "    # y_pred=h_relu.matmul(w2)\n",
    "    # 중간단계들에 reference를 가지고 있을 필요가없으므로 한번에 구한다\n",
    "    y_pred=x.mm(w1).clamp(min=0).mm(w2)\n",
    "    # loss 를 계산하고 출력한다\n",
    "    loss=(y_pred-y).pow(2).sum()\n",
    "    if t%10==0:\n",
    "        print(t,loss.item())\n",
    "    # 손실에 따른 w1,w2의 변화도를 계산하고 역전파합니다\n",
    "    # grad_y_pred=2.0*(y_pred-y)\n",
    "    # grad_w2=h_relu.t().matmul(grad_y_pred)\n",
    "    # grad_h_relu=grad_y_pred.matmul(w2.t())\n",
    "    # grad_h=grad_h_relu.clone()\n",
    "    # grad_h[h<0]=0\n",
    "    # grad_w1=x.t().matmul(grad_h)\n",
    "    loss.backward()\n",
    "    # 경사하강법을 사용하여 가중치를 갱신한다.\n",
    "    with torch.no_grad():\n",
    "        w1-=LR*w1.grad\n",
    "        w2-=LR*w2.grad\n",
    "        #가중치 갱신후에는 수동으로 변화도를 0으로 만들어준다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의합니다.\n",
    "# nn.Sequential은 다른 Module들을 포함하는 Module로, 그 Module들을 순차적으로\n",
    "# 적용하여 출력을 생성합니다. 각각의 Linear Module은 선형 함수를 사용하여\n",
    "# 입력으로부터 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# 또한 nn 패키지에는 널리 사용하는 손실 함수들에 대한 정의도 포함하고 있습니다;\n",
    "# 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다. Module 객체는\n",
    "    # __call__ 연산자를 덮어써(override) 함수처럼 호출할 수 있게 합니다.\n",
    "    # 이렇게 함으로써 입력 데이터의 Tensor를 Module에 전달하여 출력 데이터의\n",
    "    # Tensor를 생성합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다. 예측한 y와 정답인 y를 갖는 Tensor들을 전달하고,\n",
    "    # 손실 함수는 손실 값을 갖는 Tensor를 반환합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계를 실행하기 전에 변화도를 0으로 만듭니다.\n",
    "\n",
    "    # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를\n",
    "    # 계산합니다. 내부적으로 각 Module의 매개변수는 requires_grad=True 일 때\n",
    "    # Tensor 내에 저장되므로, 이 호출은 모든 모델의 모든 학습 가능한 매개변수의\n",
    "    # 변화도를 계산하게 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다. 각 매개변수는\n",
    "    # Tensor이므로 이전에 했던 것과 같이 변화도에 접근할 수 있습니다.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델과 손실 함수를 정의합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# optim 패키지를 사용하여 모델의 가중치를 갱신할 Optimizer를 정의합니다.\n",
    "# 여기서는 Adam을 사용하겠습니다; optim 패키지는 다른 다양한 최적화 알고리즘을\n",
    "# 포함하고 있습니다. Adam 생성자의 첫번째 인자는 어떤 Tensor가 갱신되어야 하는지\n",
    "# 알려줍니다.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계 전에, Optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인)\n",
    "    # 갱신할 변수들에 대한 모든 변화도를 0으로 만듭니다. 이렇게 하는 이유는\n",
    "    # 기본적으로 .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고)\n",
    "    # 누적되기 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를\n",
    "    # 참조하세요.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 매개변수에 대한 손실의 변화도를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in,H,D_out):\n",
    "        super(TwoLayerNet,self).__init__()\n",
    "        self.linear1=torch.nn.Linear(D_in,H)\n",
    "        self.linear2=torch.nn.Linear(H,D_out)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        '''순전파 함수에서는 입력 데이터의 tensor를 받고 출력 데이터의 tensor를 반환해야 한다.\n",
    "        Tensor상의 임의의 연산자뿐만 아니라 생성자에서 정의한 Module도 사용할 수 있다.'''\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred=self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "### pytorch \n",
    "- 제어 흐름(Contorl Flow)+가중치공유(weight sharing)\n",
    "```\n",
    "동적 그래프와 가중치 공유의 예로, 매우 이상한 모델을 구현해보겟다.각 순전파 단계에서 많은 은닉 계층을 갖는 완전히 연결(fully connected)된 Relu신경망이 무자구이로 0~3 사이의 숫자를 선택하고, 가장 안쪽(innerMost)의 \n",
    "은닉층들을 계산하기 위해 동일한 가중치를 여러 번 재사용합니다 \n",
    "\n",
    "이 모델에서는 일반적인 Python 제어 흐름을 사용하여 반복을 구현할 수 있으며, 순전파 단계를 정의할 때 단지 동일한 module 을 여러번 재사용함으로써 내부계층들 간의 가중치 공유를 구현할 수 있습니다.\n",
    "이러한 모델을 Moduel을 상속받는 서브클래스로 간단히 구현해보겟습니다.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in,H,D_out):\n",
    "        super(DynamicNet,self).__init__()\n",
    "        self.input_linear=torch.nn.Linear(D_in,H)\n",
    "        self.middle_linear=torch.nn.Linear(H,H)\n",
    "        self.output_linear=torch.nn.Linear(H,D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu=self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0,3)):\n",
    "            h_relu=self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred=self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "N,D_in,H,D_out=64,1000,100,10\n",
    "\n",
    "x=torch.randn(N,D_in)\n",
    "y=torch.randn(N,D_out)\n",
    "\n",
    "medel=DynamicNet(D_in,H,D_out)\n",
    "\n",
    "# 손실함수와 옵티마이저 생성 \n",
    "# -> 이 요상한 모델은 SGD로 구할 수 없으니 momentum을 이용한다고 한다.\n",
    "criterion=torch.nn.MSELoss(reduction='sum')\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-4,momentum=0.9)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred=model(x)\n",
    "    loss=criterion(y_pred,y)\n",
    "    if t%100==0:\n",
    "        print(t,loss.item())\n",
    "    # 변화도를 0으로 만들고 역전파 단계를 수행하고 가중치를 갱신한다.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "### MNiST 데이터 준비 \n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "Data_path=Path('data')\n",
    "PATH=Data_path/'mnist'\n",
    "PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "URL='https://deeplearning.net/data/mnist/'\n",
    "FileName='mnist.pkl.gz'\n",
    "\n",
    "if not(PATH/FileName).exists():\n",
    "    content=requests.get(URL+FileName).content\n",
    "    (PATH/FileName).open('wb').write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "data_train,data_test=tf.keras.datasets.mnist.load_data()\n",
    "image_train,label_train=data_train\n",
    "image_test,label_test=data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(image_train[2].reshape((28,28)),cmap='gray')\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x_train,y_train=data_train\n",
    "x_vaild,y_vaild=data_test\n",
    "x_train,y_train,x_vaild,y_vaild=map(torch.tensor,(x_train,y_train,x_vaild,y_vaild))\n",
    "print( x_train.shape)\n",
    "x_train,x_train.shape,y_train.min(),y_train.max()\n",
    "print(x_train,y_train)\n",
    "print(x_train.reshape(-1,28*28).shape)\n",
    "print(y_train.min(),y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "w1= torch.randn(784,10)/math.sqrt(784)\n",
    "w1.requires_grad_()\n",
    "bias=torch.zeros(10,requires_grad=True)\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x-x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb.mm(w1)+bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64\n",
    "xb=x_train.reshape(-1,(28*28))[0:bs]\n",
    "print(xb.shape)\n",
    "preds=model(xb)\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "source": [
    "###TensorBoard 로 모델 , 데이터 , 학습 시각화하기\n",
    "---\n",
    "\n",
    "1. 데이터를 읽고 적적히 변화합니다.\n",
    "2. TensorBoard를 set up 합니다.\n",
    "3. TensorBoard에 write 합니다\n",
    "4. TensorBoard를 사용하여 모델 구조를 살펴봅니다\n",
    "5. 약가의 코드를 추가하여 TensorBoard에서 이전 듀토리얼에서 만든 시각화의 대화식 버전을 만듭니다\n",
    "---\n",
    "- 학습데이터를 검사하는 몇가지 방법\n",
    "- 학습에 따른 모델의 성능을 추적하는 방법\n",
    "- 학습이 완료된 모델의 성능 평가하는 방법\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "# transform \n",
    "transform =transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "#datasets\n",
    "trainset=torchvision.datasets.FashionMNIST('./data',download=True,train=True,transform=transform)\n",
    "testset=torchvision.datasets.FashionMNIST('./data',download=True,train=False,transform=transform)\n",
    "#dataloaders\n",
    "trainloader=torch.utils.data.DataLoader(trainset,batch_size=4,shuffle=True,num_workers=2)\n",
    "testloader=torch.utils.data.DataLoader(testset,batch_size=4,shuffle=False,num_workers=2)\n",
    "\n",
    "#분류 결과를 위한 상수 \n",
    "classes=('T-shirt/top','Trouse','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Bot')\n",
    "#이미지를 보여주기 위한 헬퍼 함수\n",
    "def matplotlib_imshow(img,one_channel=False):\n",
    "    if one_channel:\n",
    "        img=img.mean(dim=0)\n",
    "    img=img/2+0.5\n",
    "    npimg=img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg,cmap='Greys')\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg,(1,2,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*4*4,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,16*4*4)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net=Net()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard 설정\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# 기본 log_dir 은 runs이며 여기서는 더 구체적으로 지정하였다. \n",
    "writer=SummaryWriter('runs/fashion_mnist_experiment_1')\n",
    "\n",
    "# TensorBoard 에 기록하기 \n",
    "\n",
    "dataiter=iter(trainloader)\n",
    "images,labels=dataiter.next()\n",
    "#이미지 그리드를 만듭니다 \n",
    "img_grid=torchvision.utils.make_grid(images)\n",
    "\n",
    "# 이미지를 보여줍니다.\n",
    "matplotlib_imshow(img_grid,one_channel=True)\n",
    "\n",
    "# 텐서보드에 기록합니다 \n",
    "writer.add_image('four_fashion_mnist_images',img_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net,images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    데이터셋에서 n개의 임의의 데이터포인트(datapoint)와 그에 해당하는 라벨을 선택합니다\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# 임의의 이미지들과 정답(target) 인덱스를 선택합니다\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# 각 이미지의 분류 라벨(class label)을 가져옵니다\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# 임베딩(embedding) 내역을 기록합니다\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "source": [
    "### TEXT \n",
    "---\n",
    "- 모델 정의하기 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead,nhid, nlayers,dropout=0.5):\n",
    "        super(TransformerModel,self).__init__()\n",
    "        from torch.nn import TransformerEncoder,TransformerEncoderLayer\n",
    "        self.model_type='Transformer'\n",
    "        self.src_mask=None\n",
    "        self.pos_encoder=PositionalEncoding(ninp,dropout)\n",
    "        encoder_layers=TransformerEncoderLayer(ninp,nhead,nhid,dropout)\n",
    "        self.transformer_encoder=TransformerEncoder(encoder_layers,nlayers)\n",
    "        self.encoder=nn.Embedding(ntoken,ninp)\n",
    "        self.ninp=ninp\n",
    "        self.decoder=nn.Linear(ninp,ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self,sz):\n",
    "        mask=(torch.triu(torch.ones(sz,sz))==1).transpose(0,1)\n",
    "        mask=mask.float().masked_fill(mask==0,float('-inf')).masked_fill(mask==1,float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange=0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange,initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange,initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0)!=len(src):\n",
    "            device=src.device\n",
    "            mask=self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask=mask\n",
    "\n",
    "            src=self.encoder(src)*math.sqrt(self.ninp)\n",
    "            src=self.pos_encoder(src)\n",
    "            output=self.transformer_encoder(src,self.src_mask)\n",
    "            output=self.decoder(output)\n",
    "            return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "\n",
    "        pe=torch.zeros(max_len,d_model)\n",
    "        position=torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
    "        div_term=torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        pe=pe.unsqueeze(0).transpose(0,1)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x+self.pe[:x.size(0),:]\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "source": [
    "### 데이터 로드하고 배치 만들기\n",
    "---\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([104335, 20])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT=torchtext.data.Field(\n",
    "    tokenize=get_tokenizer('basic_english'),\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    lower=True\n",
    ")\n",
    "train_txt, val_txt,test_txt=torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device=torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "def batchify(data,bsz):\n",
    "    data=TEXT.numericalize([data.examples[0].text])\n",
    "    #데이터셋을 bsz파트들로 나눈다\n",
    "    nbatch=data.size(0)//bsz\n",
    "    #깔끔하게 나누어 떨어지지 않는 추가적인 부분은 잘라낸다\n",
    "    data=data.narrow(0,0,nbatch*bsz)\n",
    "    #데이터에 대하여 bsz배치들로 동등하게 나눈다\n",
    "    data=data.view(bsz,-1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size=20\n",
    "eval_batch_size=10\n",
    "train_data=batchify(train_txt,batch_size)\n",
    "val_data=batchify(val_txt,eval_batch_size)\n",
    "test_data=batchify(test_txt,eval_batch_size)\n",
    "train_data.shape\n"
   ]
  },
  {
   "source": [
    "\n",
    "### input과 target 시퀀스를 생성하기 위한 함수들\n",
    "---\n",
    "```\n",
    "gat_batch() 함수는 트랜스포머 모델을 위한 입력과 타켓시퀀스를 생성합니다. 이 함수는 소스 데이터를 bptt \n",
    "길이를 가진 덩어리로 세분화 합니다. 언어 모델링 과제를 위해서, 모델은 다음 단어인 target이 필요합니다 예를 \n",
    "들어, bptt 의값이 2라면 우리는 i=0 일떄 다음의 2개의 변수(variable)를 얻을 수 있습니다.\n",
    "```\n",
    "|Input ||| Target|\n",
    "|:------|------:|\n",
    "|A G M S||| B H N T|\n",
    "|B H B T||| C I O U|"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt =35\n",
    "def get_batch(source,i):\n",
    "    seq_len=min(bptt,len(source)-1-i)\n",
    "    data=source[i:i+seq_len]\n",
    "    target=source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "    "
   ]
  },
  {
   "source": [
    "### 인스턴스(instance) 초기화하기\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens=len(TEXT.vocab.stoi)# 단어 사전의 크기\n",
    "emsize=200#임베딩 차원\n",
    "nhid=200 #nn.TransformerEncoder에서 피드포워드 네트워크 모델의 차원\n",
    "nlayers= 2 \n",
    "nhead=2 # 멀티헤드 어텐션 모델의 헤드 개수 \n",
    "dropout=0.2 # 드랍아웃 값 \n",
    "model=TransformerModel(ntokens,emsize,nhead,nhid,nlayers,dropout).to(device)\n"
   ]
  },
  {
   "source": [
    "```\n",
    "PositionalEncoding 모듈은 시퀀스 안에서 토큰의 상대적인 또는 절대적인 포지션에 대한 어떤 정보를\n",
    "\n",
    "주입합닏. 포지셔널 인코딩은 임베딩과 합칠 수 있도록 같은 차원을 가집니다.\n",
    "\n",
    "여기에서 우리는 다른 주파수의 sin과 cos함수를 사용합니다 \n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 모델 실행하기 \n",
    "---\n",
    "\n",
    "```\n",
    " loss을 추적하는 데에는 `CrossEntropyLoss` 가 적용되며, 옵티마이저로서 SGD는 확률적 경사 하강법을\n",
    " 구현합니다. 초기 학습률은 5.0으로 설정됩니다.`stepLR`은 에포크에 따라서 학습률을 조절하는데 사용됩니다. \n",
    " 학습하는 동안에,우리는 기울기 폭발(gradient Exploding)을 방지하기 위하여 모든 기울기를 함께 스케일(scale)\n",
    " 하는 함수인`nn.utils.clip_grad_norm_`을 이용합니다\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # 학습률\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # 학습 모드를 시작합니다.\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # 평가 모드를 시작합니다.\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'view'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c28889c39e03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m89\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-0a7b95f8bdd2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # 에포크 수\n",
    "best_model = None\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  }
 ]
}