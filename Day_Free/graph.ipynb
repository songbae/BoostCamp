{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a955008dc820c70e8c41cf6f115bde945f96d07d69c96eec2ee76e53bea50083"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import dgl \n",
    "from dgl.data import CoraGraphDataset\n",
    "from sklearn.metrics import f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.5 \n",
    "Lr=1e-2\n",
    "epoch=50\n",
    "hd=128\n",
    "nlayers=2 \n",
    "wd=5e-4\n",
    "aggre='gcn'\n",
    "# 하이퍼 파라미터 값 설정 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  NumNodes: 2708\n  NumEdges: 10556\n  NumFeats: 1433\n  NumClasses: 7\n  NumTrainingSamples: 140\n  NumValidationSamples: 500\n  NumTestSamples: 1000\nDone loading data from cached files.\nGraph(num_nodes=2708, num_edges=10556,\n      ndata_schemes={'train_mask': Scheme(shape=(), dtype=torch.bool), 'label': Scheme(shape=(), dtype=torch.int64), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'feat': Scheme(shape=(1433,), dtype=torch.float32)}\n      edata_schemes={})\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n1433\ntensor([3, 4, 4,  ..., 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "G=CoraGraphDataset() \n",
    "numClasses=G.num_classes\n",
    "G=G[0]\n",
    "#노드들의 feauture와 dimention \n",
    "features=G.ndata['feat']\n",
    "inFD=features.shape[1]\n",
    "#각 노드들의 실제 라벨 \n",
    "labels=G.ndata['label']\n",
    "#학습/테스트 사용할 때 노드들에 대한 표시 \n",
    "trainmask=G.ndata['train_mask']\n",
    "testmask=G.ndata['test_mask']\n",
    "\n",
    "print(G)\n",
    "\n",
    "print(features)\n",
    "print(inFD)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evluateTrain(model, features, lables, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits=model(features)\n",
    "        logits=logits[mask]\n",
    "        labels=labels[mask]\n",
    "        _,indices =torch.max(logits,dim=1)\n",
    "        correct= torch.sum(indices==labels)\n",
    "        return correct.item()*1.0/len(labels)\n",
    "\n",
    "def evaluateTest(mode, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits=model(features)\n",
    "        logits=logits[mask]\n",
    "        labels=labels[mask]\n",
    "        _,indices =torch.max(logits, dim=1)\n",
    "\n",
    "        macro_f1=f1_score(labels,indices,average='macro')\n",
    "        correct= torch.sum(indices==labels)\n",
    "        return correct.item()*1.0/len(labels),macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lossfunction,features,labels,trainmask,optimizer,epoches):\n",
    "    executionTime=list()\n",
    "\n",
    "    for epochs in range(epoches):\n",
    "        model.train()\n",
    "\n",
    "        starttime=time.time()\n",
    "        logits=model(features)\n",
    "        loss=lossfunction(logits[trainmask],labels[trainmask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        executionTime.append(time.time()-starttime)\n",
    "\n",
    "        acc=evluateTrain(model,features,labels,trainmask)\n",
    "\n",
    "        print(f'epoch:{epoches} Time :{np.mean(executionTime)} Loss:{loss.item()} ACC:{acc}')\n",
    "\n",
    "def test(model,features,labels,testmask):\n",
    "    acc,macro_f1=evaluateTest(model,features,labels,testmask)\n",
    "    print('Test ACC{:.4f}'.format(acc))\n",
    "    print('Test macro_f1 {:.4f}'.format(macro_f1)\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch.conv import SAGEConv\n",
    "'''\n",
    "dropout=0.5 \n",
    "Lr=1e-2\n",
    "epoch=50\n",
    "hd=128\n",
    "nlayers=2 \n",
    "wd=5e-4\n",
    "aggre='gcn'\n",
    "'''\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,graph, infeatDIM,numHiddenDim,numClasses,numLayers,activationFuntion,dropoutProb,aggregatorType):\n",
    "        super(GraphSAGE,self).__init__()\n",
    "        self.layers=nn.ModuleList()\n",
    "        self.graph=graph\n",
    "\n",
    "        #인풋 레이어 \n",
    "        self.layers.append(SAGEConv(infeatDIM,numHiddenDim,aggregatorType,dropoutProb,activationFuntion))\n",
    "        #히든레이어 \n",
    "        for i in range(numLayers):\n",
    "            self.layers.append(SAGEConv(numHiddenDim,numHiddenDim,aggregatorType,dropoutProb,activationFuntion))\n",
    "        self.layers.append(SAGEConv(numHiddenDim,numClasses,aggregatorType,dropoutProb,activation=None))\n",
    "    \n",
    "\n",
    "    def forward(self, features):\n",
    "        x=features\n",
    "        for layer in self.layers:\n",
    "            x=layer(self.graph,x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GraphSAGE(\n  (layers): ModuleList(\n    (0): SAGEConv(\n      (feat_drop): Dropout(p=0.5, inplace=False)\n      (fc_neigh): Linear(in_features=1433, out_features=128, bias=True)\n    )\n    (1): SAGEConv(\n      (feat_drop): Dropout(p=0.5, inplace=False)\n      (fc_neigh): Linear(in_features=128, out_features=128, bias=True)\n    )\n    (2): SAGEConv(\n      (feat_drop): Dropout(p=0.5, inplace=False)\n      (fc_neigh): Linear(in_features=128, out_features=128, bias=True)\n    )\n    (3): SAGEConv(\n      (feat_drop): Dropout(p=0.5, inplace=False)\n      (fc_neigh): Linear(in_features=128, out_features=7, bias=True)\n    )\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dropout=0.5 \n",
    "Lr=1e-2\n",
    "epoch=50\n",
    "hd=128\n",
    "nlayers=2 \n",
    "wd=5e-4\n",
    "aggre='gcn'\n",
    "'''\n",
    "model= GraphSAGE(G,inFD,hd,numClasses,nlayers,F.relu,dropout,aggre)\n",
    "print(model)\n",
    "\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 초기화 \n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=Lr,weight_decay=wd)\n"
   ]
  }
 ]
}