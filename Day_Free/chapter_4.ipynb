{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### word2vec 속도개선 \n",
    "--- \n",
    "- 기존 문제점 \n",
    "\n",
    "    - 입력층의 원핫표현과 가중치 행렬 W의 곱 계산\n",
    "    - 은닉층의 가중치 행렬 W의 곱및 softmax계층의 계산 \n",
    "---\n",
    "\n",
    "- Embedding 도입 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 구현\n",
    "import numpy as np\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self,W):\n",
    "        self.params=[W]\n",
    "        self.grads=[np.zeros_like(W)]\n",
    "        self.idx=None\n",
    "    def forward(self,idx):\n",
    "        W =self.params\n",
    "        self.idx=idx\n",
    "        out=W[idx]\n",
    "        return out\n",
    "\n",
    "    # def backward(self,dout):\n",
    "    #     dw, =self.grads\n",
    "    #     dw[...]=0 #->dw가 0이 되는 것이 아니라 그 형태를 0으로 덮어씌우는 작업 사실상 np.zeros로도가능할듯\n",
    "    #     dw[self.idx]=dout\n",
    "    #     return None\n",
    "    def backward(self,dout):\n",
    "        dw,= self.grads\n",
    "        dw[...]=0\n",
    "        for i , word_id in enumerate(self.idx):\n",
    "            dw[word_id]+=dout[i]\n",
    "            #혹은 \n",
    "            #np.add.at(dw,self.idx,dout)-> 이작업은 for문 자체를 대신하는 것이다 numpy의 특성중 하나로 for문을 쓰지않고 한번에 연산을 가능하게 해주는것 ㅍㅍ\n",
    "        return None\n",
    "## 하지만 이 backward 과정에는 문제가 있다. dh-> dw를 갱신할떄 즉 weight값을 갱신해줄떄 만약에 처음 idx가 \n",
    "# 0,2,4,0 처럼 2개의 값을 갖는 경우는 기울기가 중복되어 덮어 씌어지게 된다 이를 해결하기 위해서 덮어쓰지 않고 dh의 각 행의 값을 dw의 해당행에 더해준다 ."
   ]
  },
  {
   "source": [
    "### Note\n",
    "---\n",
    "` 다중 분류 문제를 이진분류로 다루려면 '정답'과 '오답' 각각에 대해 바르게 분류할 수 있어야 한다. 따라서 \n",
    "긍정적 예와 부정적 예 모두를 대상으로 학습해야 한다`\n",
    "\n",
    "- 따라서 모든 부정적 답에 대해서 결론을 주는 것이 아니라 5~6개의 부정적 예를 샘플링해 사용한다.`네거티브샘플링`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##embedding class 구현\n",
    "\n",
    "class EmbeddingDot:\n",
    "    def __intit__(self,W):\n",
    "        self.embed=Embedding(W)\n",
    "        self.params=self.embed.params\n",
    "        self.grads=self.embed.grads\n",
    "        self.cache=None\n",
    "    \n",
    "    def forward(self,h,idx):\n",
    "        target_W=self.embed.forward(W)\n",
    "        out=np.sum(target_W*h,axis=1)\n",
    "        self.cache=(h,target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self,dout):\n",
    "        h,target_W=self.cache\n",
    "        dout=dout.reshape(dout.shape[0],1)\n",
    "\n",
    "        dtarget_W=dout*h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh=dout*target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling loss 구하기\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self,W,corpus,power=0.75,sample_size=5):\n",
    "        self.sample_size=sample_size\n",
    "        self.sampler=UnigramSampler(corpus,power,sample_size)\n",
    "        self.loss_layer=[SigmoidWithLoss() for  _ in range(sample_size+1)]\n",
    "        self.embed_dot_layers=[EmbeddingDot(W) for _ in range(sample_size+1)]\n",
    "        self.params,self.grad=[],[]\n",
    "        for layers in self.embed_dot_layers:\n",
    "            self.params+=layers.params\n",
    "            self.grads+=layers.grads"
   ]
  },
  {
   "source": [
    "\n",
    "#chapter 4 요약\n",
    "\n",
    "- Embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 벡터를 추출한다.\n",
    "\n",
    "- word2vec은 어휘 수의 증가에 비례하여 계산량도 증가하므로 근사치로 계산하는 빠른 기법을 사용한다.\n",
    "\n",
    "- 네거티브 샘플링은 부정적 예를 몇개를 샘플링하는 기법으로 이를 이용하면 다중 분류를 이진 분류처럼 취급할 수 있다. \n",
    "- word2vec으로 얻은 단어의 분산 표현에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치한다.\n",
    "\n",
    "- word2ve의 단어의 분산 표현을 이용하면 유추문제를 벡터의 덧셈과 뺄셈으로 풀 수있게된다.\n",
    "\n",
    "- word2vec 은 전이 학습 측면에서 특히 중요하며, 그 단어의 분산 표현은 다양한 자연어 처리 작업에 이용 할 수있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}